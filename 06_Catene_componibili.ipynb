{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e41fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "import os\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # carica le variabili d'ambiente dal file .env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f159f63",
   "metadata": {},
   "source": [
    "# Indovinelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "850c6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "indovinello_1 = \"Qual è quell'animale che al mattino cammina a 4 zampe, a mezzogiorno con 2, alla sera con 3?\"\n",
    "indovinello_2 = \"Cos'è quella cosa che tocca solo una persona ma ne unisce ben due?\"\n",
    "indovinello_3 = \"Cos’è che ti tiene in vita e si vede solo d’inverno?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88070311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMs\n",
    "mini_llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                      api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                      temperature=0.3)\n",
    "\n",
    "big_llm  = ChatOpenAI(model=\"gpt-5\",\n",
    "                      api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad4741a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Prompt 1: il modello piccolo prova a indovinare ===\n",
    "# Chiediamo un output JSON strutturato per evitare ambiguità.\n",
    "\n",
    "riddle_solver_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Sei un risolutore di indovinelli.\n",
    "Indovinello: {riddle}\n",
    "\n",
    "Fornisci la risposta indicando:\n",
    "- \"ipotesi\": la risposta all'indovinello in UNA o poche parole.\n",
    "- \"spiegazione\": una breve spiegazione (max 5 parole).\n",
    "\n",
    "Rispondi SOLO seguendo queste istruzioni\"\"\"\n",
    ")\n",
    "\n",
    "riddle_solver_chain = riddle_solver_prompt | mini_llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8317e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- \"ipotesi\": anello\\n- \"spiegazione\": unisce due persone in matrimonio.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "riddle_solver_chain.invoke({\"riddle\": indovinello_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67a75cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Prompt 2: il modello grande vede solo l'output del primo ===\n",
    "# Obiettivo: ricostruire l’indovinello originale avendo soltanto ipotesi+spiegazione.\n",
    "reconstruction_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Hai a disposizione soltanto questa traccia, prodotta da un modello più piccolo:\n",
    "\n",
    "{mini_output}\n",
    "\n",
    "Prova a ricostruire l'indovinello ORIGINALE.\n",
    "Se necessario, formula il miglior indovinello plausibile coerente con la traccia.\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb1505e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catena composita\n",
    "complete_chain = {\n",
    "    \"mini_output\": riddle_solver_chain  # passa direttamente l'output JSON del primo LLM\n",
    "} | reconstruction_prompt | big_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "384f0198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senza inizio né fine, brillo ma non sono stella. Al dito prometto “per sempre” e unisco due cuori. Chi sono?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_chain.invoke({\"riddle\": indovinello_2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70f01a",
   "metadata": {},
   "source": [
    "# Sintassi LangChain\n",
    "Elementi:\n",
    "- complete_chain\n",
    "    - *riddle_solver_chain*:\n",
    "        - prompt\n",
    "        - mini_llm\n",
    "        - StrOutputParser\n",
    "    - reconstruction_prompt\n",
    "    - big_llm\n",
    "    - StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "067e9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "riddle_solver_chain = riddle_solver_prompt | mini_llm | StrOutputParser()\n",
    "complete_chain = riddle_solver_chain | reconstruction_prompt | big_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faa06d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_chain = (riddle_solver_prompt\n",
    "                | mini_llm \n",
    "                | StrOutputParser()\n",
    "                | reconstruction_prompt\n",
    "                | big_llm\n",
    "                | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42340500",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_chain = (riddle_solver_prompt\n",
    "                 .pipe(mini_llm)\n",
    "                 .pipe(StrOutputParser())\n",
    "                 .pipe(reconstruction_prompt)\n",
    "                 .pipe(big_llm)\n",
    "                 .pipe(StrOutputParser()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "riddle_solver_chain = riddle_solver_prompt | mini_llm | StrOutputParser()\n",
    "\n",
    "complete_chain = (\n",
    "    {\"mini_output\": riddle_solver_chain}  # dict => RunnableMap/Parallel implicito\n",
    "    | reconstruction_prompt\n",
    "    | big_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "complete_chain.invoke({\"riddle\": indovinello_3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a16acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_result = riddle_solver_chain.invoke({\"riddle\": indovinello_3})\n",
    "final_result = (\n",
    "    reconstruction_prompt\n",
    "    | big_llm\n",
    "    | StrOutputParser()\n",
    ").invoke({\"mini_output\": mini_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rinomina l'output in {\"mini_output\": ...}\n",
    "as_mini_output = RunnableLambda(lambda x: {\"mini_output\": x})\n",
    "\n",
    "complete_chain = (\n",
    "    riddle_solver_chain\n",
    "    | as_mini_output\n",
    "    | reconstruction_prompt\n",
    "    | big_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "complete_chain.invoke({\"riddle\": indovinello_3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1b183",
   "metadata": {},
   "source": [
    "# Content Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098221c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Chain a singolo input: THEME\n",
    "# Accresce il contesto e lancia in parallelo:\n",
    "#  - image prompt -> image (Runway)\n",
    "#  - post\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from runwayml import RunwayML, TaskFailedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7757c6d",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a73b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Config (solo theme in input)\n",
    "# -----------------------------\n",
    "REFERENCE_IMAGE_URL = \"https://www.videocitta.com/wp-content/uploads/2024/05/Andrea-Moccia-scaled.jpg\"\n",
    "PERSON_TAG = \"Andrea\"\n",
    "RUNWAY_MODEL = \"gen4_image\"\n",
    "RUNWAY_RATIO = \"1920:1080\"\n",
    "\n",
    "# ENV richieste\n",
    "if not os.getenv(\"openai_api_key\"):\n",
    "    raise RuntimeError(\"Imposta OPENAI_API_KEY.\")\n",
    "if not os.getenv(\"runway_api_key\"):\n",
    "    raise RuntimeError(\"Imposta RUNWAY_API_KEY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee4884",
   "metadata": {},
   "source": [
    "## Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Modelli\n",
    "# -----------------------------\n",
    "increasing_llm = ChatOpenAI(model=\"gpt-4o\",\n",
    "                            api_key=os.getenv(\"openai_api_key\"),\n",
    "                            temperature=0.7)\n",
    "\n",
    "post_llm       = ChatOpenAI(model=\"gpt-4o\",\n",
    "                            api_key=os.getenv(\"openai_api_key\"),\n",
    "                            temperature=0.9)\n",
    "\n",
    "# # Ollama locale per image prompt\n",
    "# image_prompt_llm = ChatOllama(\n",
    "#     model=\"llama-vincent\",\n",
    "#     temperature=0.7,\n",
    "#     num_predict=1024,\n",
    "#     base_url=\"http://localhost:11434\",\n",
    "#     request_timeout=30,\n",
    "# )\n",
    "\n",
    "# GPT-3.5-turbo fine-tuned\n",
    "image_prompt_llm = ChatOpenAI(model=\"ft:gpt-3.5-turbo-1106:personal:photorealism:BqgCFSkM\",\n",
    "                            api_key=os.getenv(\"openai_api_key\"),\n",
    "                            temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd7d065d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Create an image of a spacecat, floating in zero gravity, with stars and nebulae in the background, wearing a futuristic astronaut suit, and a small jetpack on its back. The cat's fur is a mix of dark purple and deep blue, with glowing blue stripes running along its body, and bright yellow eyes reflecting the stars. Its ears are pointed, and it sports a thin, glowing ring around its neck, similar to a high-tech collar. The cat is reaching out towards a floating, holographic fish, with its tail swishing back and forth, creating small ripples in the surrounding stardust.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 126, 'prompt_tokens': 25, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'ft:gpt-3.5-turbo-1106:personal:photorealism:BqgCFSkM', 'system_fingerprint': None, 'id': 'chatcmpl-C7UIIOhXCdYLUI4DQlOhrwkkK3etk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--af2fc581-19e5-4d91-ab5d-e426aab442f9-0', usage_metadata={'input_tokens': 25, 'output_tokens': 126, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_prompt_llm.invoke(\"Crea un prompt per generare un'immagine di un gatto spaziale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd217b",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt per aumentare il contesto a partire dal tema\n",
    "increasing_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Dato il tema «{theme}», elenca (bullet sintetici) le caratteristiche del tono \"\n",
    "    \"per un articolo di divulgazione scientifica destinato a un pubblico generalista. \"\n",
    "    \"Enfatizza gli aspetti più ingaggianti.\"\n",
    ")\n",
    "\n",
    "# Prompt per generare il post\n",
    "post_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Contesto ampliato (tono e spunti):\\n***\\n{context}\\n***\\n\"\n",
    "    \"Scrivi un post social molto coinvolgente (~120 parole) coerente con il tema. Call to action finale\"\n",
    "    \"Chiudi con 2–3 hashtag pertinenti.\"\n",
    ")\n",
    "\n",
    "# Prompt per generare il prompt dell'immagine\n",
    "image_prompt_requester = ChatPromptTemplate.from_template(\n",
    "    \"Forniscimi un prompt per modificare la persona nell'immagine allegata nello stile «{theme}».\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d35999d",
   "metadata": {},
   "source": [
    "## Catena semplice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e4ee1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Increasing: theme -> context (string)\n",
    "context_chain = (\n",
    "    RunnableLambda(lambda theme: {\"theme\": theme}) # passa avanti il tema\n",
    "    | increasing_prompt     # genera il prompt per il contesto  (quello con cui abbiamo fin-tunato il modello)\n",
    "    | increasing_llm        # genera il prompt\n",
    "    | StrOutputParser()     # estrae il testo\n",
    ")\n",
    "\n",
    "# 2) Post: usa solo il context\n",
    "post_chain = (\n",
    "    RunnableLambda(lambda d: {\"context\": d[\"context\"]})  # utile per riportare il contesto in output\n",
    "    | post_prompt\n",
    "    | post_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 3) Image prompt: usa theme + context\n",
    "image_prompt_chain = (\n",
    "    RunnableLambda(lambda d: {\"theme\": d[\"theme\"]})\n",
    "    | image_prompt_requester\n",
    "    | image_prompt_llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea36c3",
   "metadata": {},
   "source": [
    "## Runway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0af338e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Runway: genera immagine a partire dall'image_prompt\n",
    "def _to_image(inputs: dict) -> dict:\n",
    "    client = RunwayML(api_key=os.getenv(\"runway_api_key\"))\n",
    "    image_prompt = inputs[\"image_prompt\"]\n",
    "    try:\n",
    "        task = client.text_to_image.create(\n",
    "            model=RUNWAY_MODEL,\n",
    "            ratio=RUNWAY_RATIO,\n",
    "            prompt_text=image_prompt,\n",
    "            reference_images=[{\"uri\": REFERENCE_IMAGE_URL,\n",
    "                               \"tag\": PERSON_TAG\n",
    "                               }],\n",
    "        ).wait_for_task_output()\n",
    "        url = task.output[0] if getattr(task, \"output\", None) else None\n",
    "        return {**inputs, \n",
    "                \"image_prompt\": image_prompt,\n",
    "                \"image_url\": url}\n",
    "    except TaskFailedError as e:\n",
    "        return {**inputs, \"image_url\": None, \"image_error\": f\"{getattr(e, 'task_details', str(e))}\"}\n",
    "\n",
    "runway_node = RunnableLambda(_to_image)\n",
    "runway_chain = (image_prompt_chain \n",
    "                | RunnableLambda(lambda p: {\"image_prompt\": p}) \n",
    "                | runway_node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886a60b",
   "metadata": {},
   "source": [
    "## Orchestrazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d6ffb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Orchestrazione\n",
    "# -----------------------------\n",
    "# Step A: preparazione condivisa — da theme → {theme, context}\n",
    "prepare = RunnableParallel(\n",
    "    theme=RunnablePassthrough(),            # passa il theme così com'è\n",
    "    context=context_chain                   # elabora il context dal theme\n",
    ")\n",
    "\n",
    "# Step B: rami paralleli finali\n",
    "# - ramo post: prende il context e scrive il post\n",
    "# - ramo immagine: genera image_prompt (theme+context) e poi crea l'immagine\n",
    "final = RunnableParallel(\n",
    "    context=itemgetter(\"context\"),          # utile riportare il contesto in output\n",
    "    post=(post_chain),\n",
    "    image=(runway_chain),\n",
    ")\n",
    "\n",
    "# Catena completa: theme -> prepare -> parallelo post+immagine\n",
    "chain = prepare | final\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be64e2e2",
   "metadata": {},
   "source": [
    "## Avvio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "971a0ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONTEXT (accresciuto) ===\n",
      " - **Equilibrato**: Mantenere un tono che bilanci entusiasmo e cautela riguardo all'avvento delle intelligenze artificiali.\n",
      "- **Accessibile**: Utilizzare un linguaggio semplice e chiaro per spiegare concetti complessi, evitando il gergo tecnico.\n",
      "- **Curiosità**: Stimolare l'interesse e la curiosità dei lettori verso il futuro tecnologico e le sue potenzialità.\n",
      "- **Entusiasmo**: Riflettere l'energia e l'eccitazione per le innovazioni nel campo dell'IA e le loro possibili applicazioni.\n",
      "- **Realismo**: Evidenziare anche le preoccupazioni e i rischi associati all'IA, senza cadere nel sensazionalismo.\n",
      "- **Immaginativo**: Evocare immagini futuristiche e scenari cyber-punk per coinvolgere la fantasia dei lettori.\n",
      "- **Riflessivo**: Invitare il pubblico a riflettere sulle implicazioni etiche e sociali delle tecnologie emergenti.\n",
      "- **Ottimista**: Sottolineare le opportunità positive che l'IA potrebbe offrire alla società, promuovendo un senso di speranza.\n",
      "- **Interattivo**: Incoraggiare il dialogo e la partecipazione del pubblico attraverso domande e spunti di riflessione.\n",
      "- **Critico**: Mantenere uno sguardo critico sulle narrazioni utopistiche e distopiche, esaminando le diverse prospettive.\n",
      "\n",
      "=== POST ===\n",
      " Immaginate un futuro dove le intelligenze artificiali lavorano al nostro fianco, trasformando la nostra quotidianità in modi che oggi sembrano fantascienza. Siamo a un bivio entusiasmante: da un lato, l'IA ci offre opportunità straordinarie, come cure mediche personalizzate e città smart; dall'altro, ci chiama a riflettere su responsabilità etiche e sociali. Siamo pronti a esplorare questo nuovo mondo? Quali scenari futuristici vi affascinano di più? E quali timori vorreste affrontare? Uniamoci in questo viaggio di scoperta, con curiosità e spirito critico. Dopo tutto, il futuro è una tela bianca: dipingiamolo insieme, con saggezza e speranza. 🌟🤖\n",
      "\n",
      "#FuturoTech #IAResponsabile #InnovazioneConEtica\n",
      "\n",
      "=== IMAGE PROMPT ===\n",
      " Transform the person in the provided image into a hyper-realistic cyber-futuristic character, heavily influenced by cyber-punk and tech aesthetics. The character should evoke the feeling of an individual from a world where AI has seamlessly integrated into society, with elements such as glowing tech implants, intricate cybernetic enhancements, and a fusion of human and machine features. The overall style should be reminiscent of high-end sci-fi concept art, with meticulous attention to detail in skin texture, facial expressions, and clothing, while maintaining a photorealistic quality.\n",
      "\n",
      "=== IMAGE URL ===\n",
      " https://dnznrvs05pmza.cloudfront.net/5b2bf95e-2fa1-4a49-bd82-a0b05671d0ee.png?_jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXlIYXNoIjoiNTlmNDZkZjFhN2YxZWE3ZCIsImJ1Y2tldCI6InJ1bndheS10YXNrLWFydGlmYWN0cyIsInN0YWdlIjoicHJvZCIsImV4cCI6MTc1NTk5MzYwMH0.YbCFcKSplWBzjakRNrqBJ_BhebdxBvxkaemOGrEJMu8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "theme = \"L'avvento delle Intelligenze Artificiali. Cyber-futurismo, Tech, Cyber-punk\"\n",
    "result = chain.invoke(theme)\n",
    "\n",
    "print(\"\\n=== CONTEXT (accresciuto) ===\\n\", result[\"context\"])\n",
    "print(\"\\n=== POST ===\\n\", result[\"post\"])\n",
    "print(\"\\n=== IMAGE PROMPT ===\\n\", result[\"image\"].get(\"image_prompt\"))\n",
    "print(\"\\n=== IMAGE URL ===\\n\", result[\"image\"].get(\"image_url\"))\n",
    "if result[\"image\"].get(\"image_error\"):\n",
    "    print(\"\\n[Runway ERROR]\\n\", result[\"image\"][\"image_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d0fe2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first={\n",
      "  theme: RunnablePassthrough(),\n",
      "  context: RunnableLambda(lambda theme: {'theme': theme})\n",
      "           | ChatPromptTemplate(input_variables=['theme'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['theme'], input_types={}, partial_variables={}, template='Dato il tema «{theme}», elenca (bullet sintetici) le caratteristiche del tono per un articolo di divulgazione scientifica destinato a un pubblico generalista. Enfatizza i sentimenti legati al tema.'), additional_kwargs={})])\n",
      "           | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000026E92365F70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000026E923647D0>, root_client=<openai.OpenAI object at 0x0000026E92367200>, root_async_client=<openai.AsyncOpenAI object at 0x0000026E92367860>, model_name='gpt-4o', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "           | StrOutputParser()\n",
      "} middle=[] last={\n",
      "  context: RunnableLambda(itemgetter('context')),\n",
      "  post: RunnableLambda(lambda d: {'context': d['context']})\n",
      "        | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Contesto ampliato (tono e spunti):\\n***\\n{context}\\n***\\nScrivi un post social molto coinvolgente (~120 parole) coerente con il tema. Chiudi con 2–3 hashtag pertinenti.'), additional_kwargs={})])\n",
      "        | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000026E92366B40>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000026E923DCA10>, root_client=<openai.OpenAI object at 0x0000026E92364770>, root_async_client=<openai.AsyncOpenAI object at 0x0000026E92364260>, model_name='gpt-4o', temperature=0.9, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "        | StrOutputParser(),\n",
      "  image: RunnableLambda(lambda d: {'theme': d['theme']})\n",
      "         | ChatPromptTemplate(input_variables=['theme'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['theme'], input_types={}, partial_variables={}, template=\"Forniscimi un prompt per modificare la persona nell'immagine allegata nello stile «{theme}» e in maniera iper-realistica.\"), additional_kwargs={})])\n",
      "         | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000026E923E7920>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000026E923DFF80>, root_client=<openai.OpenAI object at 0x0000026E923E5040>, root_async_client=<openai.AsyncOpenAI object at 0x0000026E923DFA70>, model_name='ft:gpt-3.5-turbo-1106:personal:photorealism:BqgCFSkM', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "         | StrOutputParser()\n",
      "         | RunnableLambda(...)\n",
      "         | RunnableLambda(_to_image)\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
