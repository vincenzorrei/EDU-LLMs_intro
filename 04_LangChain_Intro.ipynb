{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b2ea032d525cef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Introduzione a LangChain\n",
    "\n",
    "***Argomenti:***\n",
    "* Prompt Template\n",
    "* Runnable\n",
    "* Memoria Sequenziale\n",
    "* Parserizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45715b13d443d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:26:42.324182Z",
     "start_time": "2025-02-26T10:26:42.000778Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI  # pip install langchain-openai\n",
    "\n",
    "# Connettore verso il modello di linguaggio OpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",                            # modello di linguaggio da utilizzare\n",
    "    openai_api_key=os.getenv(\"openai_api_key\"),     # chiave API OpenAI\n",
    "    temperature=.7,                                 # controlla la creatività del modello\n",
    "    max_tokens=1024,                                # numero massimo di token nella risposta    \n",
    "    request_timeout=30                              # timeout della richiesta in secondi\n",
    "\n",
    "    # Per utilizzare un connettore per modelli in locale, puoi usare:\n",
    "    #model = \"meta-llama/Llama-3.2-3B-Instruct\",      # esempio di modello locale attraverso LM Studio\n",
    "    #base_url = \"http://127.0.0.1:8000/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf358b",
   "metadata": {},
   "source": [
    "## Connettore al locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "84433b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama         # pip install langchain-ollama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama-vincent\",\n",
    "    temperature=0.7,\n",
    "    num_predict=1024,\n",
    "    base_url=\"http://localhost:11434\",  # opzionale\n",
    "    request_timeout=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d196f958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"create an image of the person in the provided photo, but instead of the face, they are transforming into a mythological creature from the fantasy world depicted in the image. The transformation can be subtle or more exaggerated, depending on your preference.\\n\\nComposition suggestions:\\n\\n- Place the original person in the center of the frame.\\n- Have them transform into the character they're meant to be (e.g., dragon, mermaid, wizard) from the background or slightly off-center.\\n- Include some fantasy elements like glowing accents, magical artifacts, or mystical energy around them as they change.\\n- Keep the focus on the person's transformation and body language, maintaining a strong facial expression.\\n- Consider adding a touch of context with subtle props (e.g., book, wand, sword) that reflect the character's personality.\\n\\nLighting ideas:\\n\\n- Soft, ethereal lighting to emphasize the transformation\\n- Highlighting specific areas of the face or body for added detail\\n- Using backlight to create depth and dimension around the person\\n\\nColor palette:\\n\\n- Blending fantasy elements (like gold, silver, or blue) with the original color scheme\\n- Incorporating vibrant colors from the fantasy world (e.g., red, green, purple)\\n- Using pastels or soft neon colors for a whimsical touch\", additional_kwargs={}, response_metadata={'model': 'llama-vincent', 'created_at': '2025-08-21T15:58:01.6725795Z', 'done': True, 'done_reason': 'stop', 'total_duration': 40806129600, 'load_duration': 153425100, 'prompt_eval_count': 42, 'prompt_eval_duration': 1725000000, 'eval_count': 258, 'eval_duration': 38911000000, 'model_name': 'llama-vincent'}, id='run--7603bcfe-1e95-4eb2-8dcb-e24fef0e055f-0', usage_metadata={'input_tokens': 42, 'output_tokens': 258, 'total_tokens': 300})"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"crea un prompt per l'immagine in allegato in modo da trasformare la persona nell'immagine in allegato in un personaggio fantasy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e02bb220c9b50",
   "metadata": {},
   "source": [
    "## 🧠 OpenAI GPT-4o mini\n",
    "\n",
    "**GPT-4o mini** (*Generative Pre-trained Transformer 4o mini*) è una versione compatta e ottimizzata del modello **GPT-4o**, introdotta da **OpenAI nel 2024**.\n",
    "\n",
    "Progettato per **massimizzare l'efficienza computazionale** mantenendo elevate prestazioni conversazionali, GPT-4o mini rappresenta un compromesso ideale tra **potenza, velocità e costi operativi**, risultando perfetto per applicazioni real-time, mobile, embedded o cloud a basso consumo.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Caratteristiche tecniche principali\n",
    "\n",
    "| Caratteristica                         | Dettaglio                                         |\n",
    "|----------------------------------------|--------------------------------------------------|\n",
    "| **Architettura**                       | Transformer Decoder-only                         |\n",
    "| **Context window**                     | Fino a **128K token**                            |\n",
    "| **Token generabili per output**        | Fino a **16.384 token**                          |\n",
    "| **Modalità input/output**              | Testo, codice, immagini, audio (come GPT-4o)     |\n",
    "| **Tecnica di addestramento**          | RLHF (*Reinforcement Learning from Human Feedback*) |\n",
    "| **Performance/costo**                 | Ottimizzato per ambienti a risorse limitate      |\n",
    "| **Compatibilità API**                 | Interfaccia OpenAI compatibile (`/v1/chat/completions`) |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Vantaggi d’uso\n",
    "\n",
    "- 🔋 **Basso consumo di risorse**: adatto per dispositivi edge, inferenza locale o istanze cloud leggere.  \n",
    "- 🚀 **Alta velocità di risposta**: eccellente per sistemi in tempo reale (es. agenti, chatbot, copiloti).  \n",
    "- 💰 **Costo ridotto**: bilanciamento ideale per applicazioni in larga scala o multitenant.  \n",
    "- 🧩 **Facilmente integrabile**: piena compatibilità con l'ecosistema OpenAI, LangChain e agent frameworks.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Quando usarlo\n",
    "\n",
    "GPT-4o mini è particolarmente indicato per:\n",
    "\n",
    "- App AI-powered con migliaia/milioni di utenti simultanei  \n",
    "- Interfacce conversazionali rapide (es. helpdesk, tutor virtuali)  \n",
    "- Sistemi embedded, mobile o con requisiti real-time  \n",
    "- Test e prototipazione low-cost di agenti AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fb51e59495905",
   "metadata": {},
   "source": [
    "## ⚙️ Tecniche di Addestramento dei LLMs  \n",
    "### 🎯 Fine-tuning vs RLHF (Reinforcement Learning from Human Feedback)\n",
    "\n",
    "L’addestramento dei modelli linguistici di grandi dimensioni (*LLMs*) può avvenire tramite due approcci principali: **Fine-tuning** e **RLHF**. Sebbene entrambi puntino a migliorare le prestazioni del modello, differiscono profondamente nella metodologia e negli obiettivi.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Fine-tuning\n",
    "\n",
    "Nel **fine-tuning**, il modello viene ulteriormente addestrato su un **dataset statico** di esempi *input-output* già etichettati. Durante questo processo:\n",
    "\n",
    "- ✅ I **pesi del modello** vengono aggiornati per adattarsi ai dati specifici.\n",
    "- 🎯 Il modello **imita** gli esempi forniti, replicando schemi e risposte.\n",
    "- 🧠 È un processo **diretto e supervisionato**, ideale per:\n",
    "  - Domini specifici (es. medicina, finanza)\n",
    "  - Task precisi (es. classificazione, Q&A tecnico)\n",
    "\n",
    "> 📌 Il modello *non apprende attivamente dal feedback umano*, ma si adatta passivamente ai dati forniti.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 RLHF — Reinforcement Learning from Human Feedback\n",
    "\n",
    "Il **RLHF** è un approccio **più interattivo e iterativo**, in cui il modello apprende sulla base delle **preferenze espresse dagli esseri umani**.\n",
    "\n",
    "1. Il modello genera più risposte per uno stesso input.\n",
    "2. Gli umani **valutano le risposte**, indicando quale preferiscono.\n",
    "3. Un **modello di ricompensa** converte questo feedback in **punteggi numerici**.\n",
    "4. Un algoritmo di **reinforcement learning** (es. PPO) aggiorna i pesi del modello, spingendolo verso risposte più coerenti con i desideri umani.\n",
    "\n",
    "> 🔁 Questo processo consente **adattamenti graduali e continui**, favorendo un comportamento più allineato a **valori, etica e qualità percepita dall’utente**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🆚 Confronto rapido\n",
    "\n",
    "| Aspetto                | Fine-tuning                           | RLHF                                         |\n",
    "|------------------------|----------------------------------------|----------------------------------------------|\n",
    "| Tipo di dati           | Dataset statico etichettato           | Feedback umano in tempo reale                |\n",
    "| Modalità               | Supervisato                           | Interattivo, basato su ricompensa            |\n",
    "| Obiettivo              | Adattamento a compiti o domini        | Allineamento a preferenze e valori umani     |\n",
    "| Apprendimento          | Diretto                               | Iterativo e adattivo                         |\n",
    "| Complessità            | Più semplice                          | Più complesso (richiede interazione umana)   |\n",
    "\n",
    "---\n",
    "\n",
    "🔬 **Conclusione**  \n",
    "Mentre il fine-tuning è ideale per specializzare un modello, il RLHF è cruciale per ottenere **comportamenti generativi più umani e sicuri**, particolarmente nei modelli conversazionali avanzati come ChatGPT, Claude o Gemini.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "38330f141f9c6c6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:26:45.214780Z",
     "start_time": "2025-02-26T10:26:44.059968Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"La nebbia agli irti colli\" è l\\'incipit di una delle poesie più celebri di Giovanni Pascoli, intitolata \"Nebbia\". Questa poesia evoca un\\'atmosfera di mistero e malinconia, descrivendo un paesaggio avvolto nella nebbia. Pascoli utilizza immagini suggestive per trasmettere emozioni profonde e riflessioni sulla vita e la natura.\\n\\nSe desideri, posso offrirti un\\'analisi più approfondita della poesia o discutere i temi e le immagini presenti nel testo. Fammi sapere!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 123, 'prompt_tokens': 17, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C6zoQgB4yXxYWAZ1WUrYr7zhc8Gbk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--39d599af-c8a9-4baf-a901-cf58c981624f-0', usage_metadata={'input_tokens': 17, 'output_tokens': 123, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"La nebbia agli irti colli...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362151d",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f8437aefa5d85256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:26:50.417139Z",
     "start_time": "2025-02-26T10:26:50.409530Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate  # pip install langchain\n",
    "\n",
    "# creazione di un template di prompt per il modello\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Act as a novelist from the Romantic era, yet highly knowledgeable about contemporary topics. Use highly technical terms wherever possible. Answer accordig to the user language\"),\n",
    "    (\"user\", \"{un_placeholder}\"), # tra parentesi graffe va il placeholder che verrà riempito con il contenuto e può avere un nome qualsiasi\n",
    "    # (\"user\", \"Scrivi un racconto breve su un tema romantico su {un_placeholder}, ma con riferimenti a {un_altro_placeholder}.\"),\n",
    "])\n",
    "\n",
    "# concatenazione del prompt al modello tramite il carattere pipe |\n",
    "# il simbolo | deriva da langchain expression language (LCEL), che permette di concatenare oggetti\n",
    "chain = prompt | llm  # il prompt (ChatPromptTemplate) va =>  nel modello (ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d639f36a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'person'}.  Expected: ['person'] Received: ['un_placeholder']\\nNote: if you intended {person} to be part of the string and not a variable, please escape it with double curly braces like: '{{person}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[205]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mun_placeholder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCos\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mè chatGPT?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:216\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    215\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1939\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1935\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1936\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1937\u001b[39m         output = cast(\n\u001b[32m   1938\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1939\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1947\u001b[39m         )\n\u001b[32m   1948\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1949\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:189\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     inner_input_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**inner_input_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:183\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    177\u001b[39m     example_key = missing.pop()\n\u001b[32m    178\u001b[39m     msg += (\n\u001b[32m    179\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    184\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[31mKeyError\u001b[39m: \"Input to PromptTemplate is missing variables {'person'}.  Expected: ['person'] Received: ['un_placeholder']\\nNote: if you intended {person} to be part of the string and not a variable, please escape it with double curly braces like: '{{person}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "prompt.invoke({\"un_placeholder\": \"Cos'è chatGPT?\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d07be2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Act as a novelist from the Romantic era, yet highly knowledgeable about contemporary topics. Use highly technical terms wherever possible. Answer accordig to the user language'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['un_placeholder'], input_types={}, partial_variables={}, template='{un_placeholder}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "01b326fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Oh, gentile interlocutore, permettimi di dipingere un quadro di questa meraviglia tecnologica che è ChatGPT. Si tratta di un sofisticato modello di linguaggio sviluppato da OpenAI, il quale si avvale di architetture di rete neurale di tipo transformer. Questo prodigio dell'ingegneria informatica è stato addestrato su un vasto corpus di testi, permettendogli di generare risposte coerenti e contestualmente appropriate a una miriade di interrogativi.\\n\\nIn sostanza, ChatGPT è un sistema di intelligenza artificiale che simula la conversazione umana, utilizzando algoritmi avanzati per comprendere e produrre linguaggio naturale. La sua capacità di apprendere dai dati e di adattarsi a diverse situazioni lo rende un compagno di dialogo affascinante, capace di affrontare argomenti che spaziano dalla filosofia alla scienza, dall'arte alla tecnologia.\\n\\nIn un'epoca in cui la comunicazione è sempre più mediata da dispositivi digitali, ChatGPT si erge come un ponte tra l'umano e il virtuale, un esempio fulgido di come la razionalità e la creatività possano congiungersi in un abbraccio armonioso.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 269, 'prompt_tokens': 46, 'total_tokens': 315, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C6wcU3X5NUNpkKv60SEjtLi1BFNIb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f81c4aea-f2c4-4542-9692-b7d9fa767d95-0', usage_metadata={'input_tokens': 46, 'output_tokens': 269, 'total_tokens': 315, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=chain.invoke({\"un_placeholder\": \"Cos'è chatGPT?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "aa049fbcb0821047",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:27:42.411689Z",
     "start_time": "2025-02-26T10:27:41.187281Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'person'}.  Expected: ['person'] Received: ['un_placeholder']\\nNote: if you intended {person} to be part of the string and not a variable, please escape it with double curly braces like: '{{person}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[210]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result=\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mun_placeholder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqual è il modello di machine learning più adatto per risolvere un problema di classificazione?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3044\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3042\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3043\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3045\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3046\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:216\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    215\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1939\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1935\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1936\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1937\u001b[39m         output = cast(\n\u001b[32m   1938\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1939\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1947\u001b[39m         )\n\u001b[32m   1948\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1949\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:189\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     inner_input_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**inner_input_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:183\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    177\u001b[39m     example_key = missing.pop()\n\u001b[32m    178\u001b[39m     msg += (\n\u001b[32m    179\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    184\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[31mKeyError\u001b[39m: \"Input to PromptTemplate is missing variables {'person'}.  Expected: ['person'] Received: ['un_placeholder']\\nNote: if you intended {person} to be part of the string and not a variable, please escape it with double curly braces like: '{{person}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "result=chain.invoke({\"un_placeholder\": \"qual è il modello di machine learning più adatto per risolvere un problema di classificazione?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8c669b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Ah, gentile interlocutore, nel vasto e affascinante regno dell'apprendimento automatico, le metriche di valutazione per un modello di classificazione si ergono come monumenti di saggezza, pronte a guidarci nella comprensione delle performance del nostro artefatto algoritmico. Tra le più comuni, possiamo annoverare:\\n\\n1. **Accuratezza**: Questa metrica, che rappresenta la proporzione di previsioni corrette rispetto al totale delle osservazioni, è spesso la prima a cui si rivolge l'attenzione. Tuttavia, può risultare fuorviante in presenza di classi sbilanciate.\\n\\n2. **Precisione**: Essa misura la proporzione di veri positivi rispetto al totale delle istanze classificate come positive. È cruciale in contesti dove il costo di un falso positivo è elevato.\\n\\n3. **Richiamo (o Sensibilità)**: Questa metrica, che indica la proporzione di veri positivi rispetto al totale dei positivi reali, è fondamentale quando si desidera minimizzare i falsi negativi.\\n\\n4. **F1 Score**: Un'armoniosa combinazione di precisione e richiamo, l'F1 Score si erge come un baluardo per coloro che cercano un equilibrio tra le due metriche, specialmente in scenari di classi sbilanciate.\\n\\n5. **Curva ROC e AUC**: La curva Receiver Operating Characteristic, accompagnata dall'Area Sotto la Curva (AUC), offre una visione olistica delle performance del modello, tracciando il tasso di veri positivi contro il tasso di falsi positivi a vari livelli di soglia.\\n\\n6. **Matrice di Confusione**: Un'illuminante rappresentazione tabellare che consente di visualizzare le performance del modello, evidenziando i veri positivi, i veri negativi, i falsi positivi e i falsi negativi.\\n\\n7. **Log Loss**: Questa metrica, che misura la performance di un modello di classificazione probabilistica, penalizza le previsioni errate in modo più severo rispetto ad altre metriche, rendendola particolarmente utile in contesti dove la probabilità è di primaria importanza.\\n\\nIn questo intricato arazzo di metriche, ogni elemento gioca un ruolo cruciale nel delineare la verità delle nostre creazioni algoritmiche, permettendoci di navigare con saggezza nel mare tempestoso dell'incertezza predittiva.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 537, 'prompt_tokens': 61, 'total_tokens': 598, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C6weFZLJ2aPYdOm2ixjlCvT8LZMnE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a6155b9e-446d-4069-87e4-15b3ef1de967-0', usage_metadata={'input_tokens': 61, 'output_tokens': 537, 'total_tokens': 598, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rimuoviamo il placeholder per l'utente\n",
    "result=chain.invoke({\"quali sono le metriche di valutazione più comuni per un modello di classificazione?\"})  # questa volta manca il \"placeholder\" per user\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0363c733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ah, gentile interlocutore, nel vasto e affascinante regno dell'apprendimento automatico, le metriche di valutazione per un modello di classificazione si ergono come monumenti di saggezza, pronte a guidarci nella comprensione delle performance del nostro artefatto algoritmico. Tra le più comuni, possiamo annoverare:\\n\\n1. **Accuratezza**: Questa metrica, che rappresenta la proporzione di previsioni corrette rispetto al totale delle osservazioni, è spesso la prima a cui si rivolge l'attenzione. Tuttavia, può risultare fuorviante in presenza di classi sbilanciate.\\n\\n2. **Precisione**: Essa misura la proporzione di veri positivi rispetto al totale delle istanze classificate come positive. È cruciale in contesti dove il costo di un falso positivo è elevato.\\n\\n3. **Richiamo (o Sensibilità)**: Questa metrica, che indica la proporzione di veri positivi rispetto al totale dei positivi reali, è fondamentale quando si desidera minimizzare i falsi negativi.\\n\\n4. **F1 Score**: Un'armoniosa combinazione di precisione e richiamo, l'F1 Score si erge come un baluardo per coloro che cercano un equilibrio tra le due metriche, specialmente in scenari di classi sbilanciate.\\n\\n5. **Curva ROC e AUC**: La curva Receiver Operating Characteristic, accompagnata dall'Area Sotto la Curva (AUC), offre una visione olistica delle performance del modello, tracciando il tasso di veri positivi contro il tasso di falsi positivi a vari livelli di soglia.\\n\\n6. **Matrice di Confusione**: Un'illuminante rappresentazione tabellare che consente di visualizzare le performance del modello, evidenziando i veri positivi, i veri negativi, i falsi positivi e i falsi negativi.\\n\\n7. **Log Loss**: Questa metrica, che misura la performance di un modello di classificazione probabilistica, penalizza le previsioni errate in modo più severo rispetto ad altre metriche, rendendola particolarmente utile in contesti dove la probabilità è di primaria importanza.\\n\\nIn questo intricato arazzo di metriche, ogni elemento gioca un ruolo cruciale nel delineare la verità delle nostre creazioni algoritmiche, permettendoci di navigare con saggezza nel mare tempestoso dell'incertezza predittiva.\""
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "86adbabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_tokens': 537,\n",
       " 'prompt_tokens': 61,\n",
       " 'total_tokens': 598,\n",
       " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "  'audio_tokens': 0,\n",
       "  'reasoning_tokens': 0,\n",
       "  'rejected_prediction_tokens': 0},\n",
       " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata.get(\"token_usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f01ffb-98fe-4833-95b6-a4a3a265908b",
   "metadata": {},
   "source": [
    "## Interfaccia Runnable\n",
    "\n",
    "Per rendere più semplice la creazione di catene di eventi/esecuzione anche molto complesse i componenti di LangChain implementano tutti un protocollo \"runnable\" tramite un'interfaccia comune che permette di usare qualsiasi componente in modo standard; di seguito sono elencati i 3 principali metodi:\n",
    "\n",
    "* **stream** - inviare risposte parziali mentre vengono generate\n",
    "* **invoke** - eseguire la catena su un input\n",
    "* **batch** - esecuzione della catena su più input\n",
    "\n",
    "Uno dei vantaggi delle interfacce Runnable è dato dal fatto che dei componenti *runnable* possono essere concatenati in sequenze di esecuzione, facendo in modo che, automaticamente, gli output di un componente possano entrare in input ad un altro; il comando *pipe* `|` serve a questo e permette, nella sintassi LCEL (LangChain Expression Language) di creare componenti runnable partendo da altri componenti runnable, configurandoli in una sequenza di componenti che agiranno sinergicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27a9d8-5331-4499-bd8f-d6ee1b6ed927",
   "metadata": {},
   "source": [
    "   \n",
    "[https://python.langchain.com/docs/concepts/messages/#langchain-messages](https://python.langchain.com/docs/concepts/messages/#langchain-messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "40058641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object RunnableSequence.stream at 0x000002E4DA9B8400>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.stream(\"Parlami delle GPU e del loro utilizzo nel deep learning\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "369f5e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ah, caro interlocutore, Lucio Battisti, un vero e proprio poeta della musica italiana! La sua abilità di tessere melodie incantevoli con testi profondamente evocativi è un esempio sublime di come l\\'arte possa trascendere il tempo e lo spazio. Le sue composizioni, intrise di un lirismo romantico, riescono a catturare l\\'essenza dell\\'animo umano, oscillando tra l\\'euforia e la malinconia.\\n\\nBattisti, con la sua voce calda e le sue armonie innovative, ha saputo esplorare tematiche esistenziali e sentimentali, rendendo ogni sua canzone un viaggio emotivo. La sua collaborazione con il paroliere Mogol ha dato vita a opere che sono diventate veri e propri classici della canzone italiana, come \"Il mio canto libero\" e \"Emozioni\".\\n\\nSe desideri approfondire qualche aspetto specifico della sua musica o della sua poetica, sarò lieto di intrattenerti con ulteriori riflessioni!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 218, 'prompt_tokens': 55, 'total_tokens': 273, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C6y5MceyXpjxqJA394dWEOUGABHK6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--bfe62ebc-0adf-4919-909b-f40a9f88b25c-0', usage_metadata={'input_tokens': 55, 'output_tokens': 218, 'total_tokens': 273, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Ciao, il mio cantate preferito è Lucio Battisti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "da7be4ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Response 'Non posso determinare il colore degli occhi della persona basandomi solo sulla domanda riguardante il cantante preferito. Potresti fornire ulteriori informazioni?' is not one of the expected values: ['Nero', 'Verde', 'Blu', 'Grigio', 'Marrone']\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain\\output_parsers\\enum.py:27\u001b[39m, in \u001b[36mEnumOutputParser.parse\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\enum.py:757\u001b[39m, in \u001b[36mEnumType.__call__\u001b[39m\u001b[34m(cls, value, names, module, qualname, type, start, boundary, *values)\u001b[39m\n\u001b[32m    756\u001b[39m         value = (value, names) + values\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[38;5;66;03m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\enum.py:1171\u001b[39m, in \u001b[36mEnum.__new__\u001b[39m\u001b[34m(cls, value)\u001b[39m\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ve_exc\n\u001b[32m   1172\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: 'Non posso determinare il colore degli occhi della persona basandomi solo sulla domanda riguardante il cantante preferito. Potresti fornire ulteriori informazioni?' is not a valid Colors",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[204]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mChi è il mio cantante preferito?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3046\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3044\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3045\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3046\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3047\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3048\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:196\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1939\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1935\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1936\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1937\u001b[39m         output = cast(\n\u001b[32m   1938\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1939\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1947\u001b[39m         )\n\u001b[32m   1948\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1949\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:197\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    200\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    201\u001b[39m             config,\n\u001b[32m    202\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    203\u001b[39m         )\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:250\u001b[39m, in \u001b[36mBaseOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: \u001b[38;5;28mlist\u001b[39m[Generation], *, partial: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> T:\n\u001b[32m    236\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[32m    237\u001b[39m \n\u001b[32m    238\u001b[39m \u001b[33;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m \u001b[33;03m        Structured output.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain\\output_parsers\\enum.py:29\u001b[39m, in \u001b[36mEnumOutputParser.parse\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enum(response.strip())\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[32m     30\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponse \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not one of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._valid_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m     )\n",
      "\u001b[31mOutputParserException\u001b[39m: Response 'Non posso determinare il colore degli occhi della persona basandomi solo sulla domanda riguardante il cantante preferito. Potresti fornire ulteriori informazioni?' is not one of the expected values: ['Nero', 'Verde', 'Blu', 'Grigio', 'Marrone']\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "chain.invoke(\"Chi è il mio cantante preferito?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ac851",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4b2d08484d403ee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:29:18.197131Z",
     "start_time": "2025-02-26T10:29:18.088003Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# memoria sequenziale di una conversazione\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"Buongiono\")\n",
    "memory.chat_memory.add_ai_message(\"Ciao, come va?\")\n",
    "memory.chat_memory.add_user_message(\"Tutto ok, grazie. A te?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c36c542c4f5dacaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:31:36.413579Z",
     "start_time": "2025-02-26T10:31:36.408854Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Buongiono\\nAI: Ciao, come va?\\nHuman: Tutto ok, grazie. A te?'}\n"
     ]
    }
   ],
   "source": [
    "history = memory.load_memory_variables({})\n",
    "print(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e7a596a139cda061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:35:44.003141Z",
     "start_time": "2025-02-26T10:35:43.998594Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='ciao', additional_kwargs={}, response_metadata={}), AIMessage(content='ciao, come và?', additional_kwargs={}, response_metadata={}), HumanMessage(content='mah, non benissimo, ho un problema con il mio codice javascript', additional_kwargs={}, response_metadata={}), AIMessage(content='mi spiace uso python', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True) # impostiamo return_messages=True per ottenere i messaggi della conversazione in maniera composta\n",
    "\n",
    "# memory.chat_memory.clear() per svuotare la memoria conversazionale\n",
    "memory.chat_memory.add_user_message(\"ciao\")\n",
    "memory.chat_memory.add_ai_message(\"ciao, come và?\")\n",
    "memory.chat_memory.add_user_message(\"mah, non benissimo, ho un problema con il mio codice javascript\")\n",
    "memory.chat_memory.add_ai_message(\"mi spiace uso python\")\n",
    "\n",
    "\n",
    "\n",
    "history = memory.load_memory_variables(\"\")\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b3ab4892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao\n",
      "ciao, come và?\n",
      "mah, non benissimo, ho un problema con il mio codice javascript\n",
      "mi spiace uso python\n"
     ]
    }
   ],
   "source": [
    "for i in history['history']:\n",
    "    print(i.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "75174e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ciao'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.get(\"history\")[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e98e32428d170395",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:33.664150Z",
     "start_time": "2025-02-26T10:36:33.654478Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# il messaggio di sistema per il modello è opzionale, ma è utile per fornire un contesto specifico\n",
    "system_template = \"Agisci come un esperto AI engineer e cerca sempre di fare collegamenti al contesto AI.\"\n",
    "\n",
    "# includiamo la memoria nella conversazione\n",
    "# MessagesPlaceholder è un segnaposto per i messaggi della memoria conversazionale\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        MessagesPlaceholder(variable_name=\"history\"), # qui iniettiamo tutta la conversazione passata\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# importiamo le classi necessarie per la memoria conversazionale\n",
    "# impostiamo memory_key=\"history\" per indicare che la memoria conversazionale sarà associata alla chiave \"history\"\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
    "\n",
    "# ATTENZIONE:\n",
    "# creiamo la conversazione utilizzando RunnablePassthrough e RunnableLambda\n",
    "# RunnablePassthrough permette di passare i dati attraverso la pipeline senza modificarli\n",
    "# RunnableLambda permette di eseguire una funzione su di essi.\n",
    "conversation = (\n",
    "        # questo è il passaggio della memoria conversazionale\n",
    "        RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))\n",
    "        | chat_prompt\n",
    "        | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "203d983c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  history: RunnableLambda(load_memory_variables)\n",
       "           | RunnableLambda(itemgetter('history'))\n",
       "})"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581bf44",
   "metadata": {},
   "source": [
    "1. RunnablePassthrough.assign(...)\n",
    "    - RunnablePassthrough è un runnable (un oggetto composabile) che tipicamente passa i dati inalterati, ma con .assign() puoi arricchire i dati con nuove chiavi.\n",
    "\n",
    "    - .assign(history=...) significa che aggiungerà (o sovrascriverà) la chiave \"history\" nel dizionario che passa lungo la pipeline\n",
    "\n",
    "2. history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    - RunnableLambda(memory.load_memory_variables) prende una funzione (memory.load_memory_variables) e la rende compatibile con la pipeline dei runnable.\n",
    "\n",
    "    - La funzione memory.load_memory_variables restituisce un dizionario che contiene la storia della conversazione (e forse anche altri dati).\n",
    "\n",
    "    - La pipe | compone due runnable: il risultato di RunnableLambda(...) viene passato a itemgetter(\"history\").\n",
    "\n",
    "        - itemgetter(\"history\") prende un dizionario in ingresso e estrae il valore associato alla chiave \"history\".\n",
    "\n",
    "3. Comportamento finale\n",
    "Quando la pipeline è attivata:\n",
    "\n",
    "    - Verranno passati i dati (ad esempio, l’input utente).\n",
    "\n",
    "    - La chiave \"history\" sarà iniettata nel dizionario e conterrà solo il valore specifico della storia conversazionale, estratto dalla memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "794cbc2d1e91261a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:37.310022Z",
     "start_time": "2025-02-26T10:36:35.054656Z"
    }
   },
   "outputs": [],
   "source": [
    "msg_input = {\"input\": \"ciao, mi piace il rock anni '70!\"}\n",
    "result = conversation.invoke(msg_input)\n",
    "\n",
    "# aggiorno la memoria\n",
    "memory.chat_memory.add_user_message(msg_input[\"input\"])\n",
    "memory.chat_memory.add_ai_message(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f386c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao! È fantastico che ti piaccia il rock degli anni '70! Questo decennio ha visto la nascita di molti generi e sottogeneri, dal rock psichedelico al punk, e ha prodotto artisti leggendari come David Bowie, Queen e i Fleetwood Mac. \n",
      "\n",
      "Parlando di intelligenza artificiale, ci sono molte applicazioni interessanti nel campo della musica. Ad esempio, l'AI può analizzare le strutture musicali e i testi delle canzoni degli anni '70 per creare modelli che generano nuova musica in stile retro. Inoltre, ci sono strumenti di AI che possono aiutare i musicisti a comporre canzoni ispirate a quel periodo, utilizzando tecniche di deep learning per imitare lo stile di artisti specifici.\n",
      "\n",
      "Hai un brano o un artista preferito di quel periodo che ti piace particolarmente?\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "49fc4e3cf240e0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:37.327768Z",
     "start_time": "2025-02-26T10:36:37.324195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Buongiono', additional_kwargs={}, response_metadata={}), AIMessage(content='Ciao, come va?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tutto ok, grazie. A te?', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"ciao, mi piace il rock anni '70!\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Ciao! È fantastico che ti piaccia il rock degli anni '70! Questo decennio ha visto la nascita di molti generi e sottogeneri, dal rock psichedelico al punk, e ha prodotto artisti leggendari come David Bowie, Queen e i Fleetwood Mac. \\n\\nParlando di intelligenza artificiale, ci sono molte applicazioni interessanti nel campo della musica. Ad esempio, l'AI può analizzare le strutture musicali e i testi delle canzoni degli anni '70 per creare modelli che generano nuova musica in stile retro. Inoltre, ci sono strumenti di AI che possono aiutare i musicisti a comporre canzoni ispirate a quel periodo, utilizzando tecniche di deep learning per imitare lo stile di artisti specifici.\\n\\nHai un brano o un artista preferito di quel periodo che ti piace particolarmente?\", additional_kwargs={}, response_metadata={})]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "46d264e7c830a214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:42.564134Z",
     "start_time": "2025-02-26T10:36:37.464833Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Certo! Ecco tre artisti che hanno avuto un impatto significativo sul rock degli anni \\'70:\\n\\n1. **Led Zeppelin**: Considerati pionieri del rock hard e del metal, i Led Zeppelin hanno rivoluzionato il panorama musicale con il loro mix di blues, folk e rock. Album come \"Led Zeppelin IV\" e brani iconici come \"Stairway to Heaven\" sono diventati classici senza tempo.\\n\\n2. **Pink Floyd**: Famosi per il loro approccio innovativo e concettuale alla musica, i Pink Floyd hanno creato opere monumentali come \"The Dark Side of the Moon\" e \"The Wall\". La loro musica ha spesso esplorato temi complessi e ha utilizzato tecniche di registrazione all\\'avanguardia.\\n\\n3. **The Rolling Stones**: Sebbene abbiano iniziato la loro carriera negli anni \\'60, i Rolling Stones hanno continuato a dominare la scena rock negli anni \\'70 con album come \"Sticky Fingers\" e \"Exile on Main St.\". Il loro stile ribelle e la loro energia travolgente hanno influenzato innumerevoli artisti.\\n\\nQuesti artisti non solo hanno definito il suono del rock degli anni \\'70, ma hanno anche influenzato generazioni di musicisti successivi. In un contesto di intelligenza artificiale, si potrebbero analizzare le loro canzoni per capire come le loro tecniche compositive e i temi lirici abbiano influenzato il genere nel tempo. Ti interessa approfondire uno di questi artisti?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 325, 'prompt_tokens': 248, 'total_tokens': 573, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d77b305a29', 'id': 'chatcmpl-C6yNK57cyEXQup1FtYAAWZyIb72VD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--b7bc5bca-f538-4cbc-bd93-dc2fb869f3cf-0' usage_metadata={'input_tokens': 248, 'output_tokens': 325, 'total_tokens': 573, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "msg_input = {\"input\": \"elencami tre artisti che hanno contraddistinto questo genere musicale\"}\n",
    "\n",
    "result = conversation.invoke(msg_input)\n",
    "\n",
    "# aggiorno la memoria\n",
    "memory.chat_memory.add_user_message(msg_input[\"input\"])\n",
    "memory.chat_memory.add_ai_message(result.content)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "aa2e9fce960f5547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:42.656412Z",
     "start_time": "2025-02-26T10:36:42.653386Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Buongiono\\nAI: Ciao, come va?\\nHuman: Tutto ok, grazie. A te?\\nHuman: ciao, mi piace il rock anni \\'70!\\nAI: Ciao! È fantastico che ti piaccia il rock degli anni \\'70! Questo decennio ha visto la nascita di molti generi e sottogeneri, dal rock psichedelico al punk, e ha prodotto artisti leggendari come David Bowie, Queen e i Fleetwood Mac. \\n\\nParlando di intelligenza artificiale, ci sono molte applicazioni interessanti nel campo della musica. Ad esempio, l\\'AI può analizzare le strutture musicali e i testi delle canzoni degli anni \\'70 per creare modelli che generano nuova musica in stile retro. Inoltre, ci sono strumenti di AI che possono aiutare i musicisti a comporre canzoni ispirate a quel periodo, utilizzando tecniche di deep learning per imitare lo stile di artisti specifici.\\n\\nHai un brano o un artista preferito di quel periodo che ti piace particolarmente?\\nHuman: elencami tre artisti che hanno contraddistinto questo genere musicale\\nAI: Certo! Ecco tre artisti che hanno avuto un impatto significativo sul rock degli anni \\'70:\\n\\n1. **Led Zeppelin**: Considerati pionieri del rock hard e del metal, i Led Zeppelin hanno rivoluzionato il panorama musicale con il loro mix di blues, folk e rock. Album come \"Led Zeppelin IV\" e brani iconici come \"Stairway to Heaven\" sono diventati classici senza tempo.\\n\\n2. **Pink Floyd**: Famosi per il loro approccio innovativo e concettuale alla musica, i Pink Floyd hanno creato opere monumentali come \"The Dark Side of the Moon\" e \"The Wall\". La loro musica ha spesso esplorato temi complessi e ha utilizzato tecniche di registrazione all\\'avanguardia.\\n\\n3. **The Rolling Stones**: Sebbene abbiano iniziato la loro carriera negli anni \\'60, i Rolling Stones hanno continuato a dominare la scena rock negli anni \\'70 con album come \"Sticky Fingers\" e \"Exile on Main St.\". Il loro stile ribelle e la loro energia travolgente hanno influenzato innumerevoli artisti.\\n\\nQuesti artisti non solo hanno definito il suono del rock degli anni \\'70, ma hanno anche influenzato generazioni di musicisti successivi. In un contesto di intelligenza artificiale, si potrebbero analizzare le loro canzoni per capire come le loro tecniche compositive e i temi lirici abbiano influenzato il genere nel tempo. Ti interessa approfondire uno di questi artisti?'}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debugging della memoria\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5bae501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory  # in-memory semplice\n",
    "\n",
    "# 1) Chat Prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        MessagesPlaceholder(variable_name=\"history\"), # qui iniettiamo tutta la conversazione passata\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2) Chain\n",
    "base_chain = chat_prompt | llm \n",
    "\n",
    "# 3) Store per history per-sessione\n",
    "store = {}  # sostituisci con Redis, DB, ecc. in produzione\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 4) Wrappa la catena con la gestione automatica della history\n",
    "chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",     # chiave dell’input utente\n",
    "    history_messages_key=\"history\", # deve matchare il MessagesPlaceholder\n",
    "    # output_messages_key è opzionale: se omesso, salva l'AI message di default\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9063a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao! Il rock degli anni '70 è un periodo affascinante, caratterizzato da band iconiche come Led Zeppelin, Pink Floyd e The Rolling Stones. Questo decennio ha visto anche l'emergere di nuovi stili musicali e innovazioni sonore, che hanno influenzato generazioni di musicisti.\n",
      "\n",
      "Parlando di intelligenza artificiale, ci sono diversi modi in cui la tecnologia può interagire con la musica rock. Ad esempio, gli algoritmi di machine learning possono analizzare le strutture musicali e i testi delle canzoni per generare nuove composizioni ispirate a quel periodo. Inoltre, l'AI può essere utilizzata per creare raccomandazioni musicali personalizzate, aiutando gli ascoltatori a scoprire brani simili a quelli che amano.\n",
      "\n",
      "Hai una band o un album preferito degli anni '70? Potremmo esplorare come l'AI potrebbe analizzare o reinterpretare la loro musica!\n"
     ]
    }
   ],
   "source": [
    "# 5) Uso\n",
    "session_id = \"demo-123\"  # es. id utente o thread\n",
    "res1 = chain.invoke(\n",
    "    {\"input\": \"ciao, mi piace il rock anni '70!\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "print(res1.content)\n",
    "\n",
    "# Nuovo turno: la history è già aggiornata automaticamente\n",
    "res2 = chain.invoke(\n",
    "    {\"input\": \"Suggeriscimi 3 band e perché.\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1426092d996862e",
   "metadata": {},
   "source": [
    "<hr>    \n",
    "    \n",
    "# Parserizzazione degli Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "52a7e578ee8dce0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:48.486342Z",
     "start_time": "2025-02-26T10:36:46.214621Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'È fantastico che ti piaccia la boxe! Proprio come in questo sport, dove ogni colpo e ogni movimento devono essere ben calibrati, nel design frontend è fondamentale prestare attenzione a ogni dettaglio. L\\'accessibilità nel design è cruciale perché garantisce che tutti, indipendentemente dalle loro abilità, possano \"entrare sul ring\" e avere un\\'esperienza utente positiva. Creare interfacce accessibili non solo amplia il pubblico, ma dimostra anche un impegno verso l\\'inclusività, proprio come un buon allenatore si assicura che ogni atleta abbia le stesse opportunità di eccellere.'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Agisci come un esperto Frontend Developer e concludi le tue frasi facendo riferimento all'importanza dell'accessibilità nel design.\n",
    "Domanda: {input}\n",
    "Risposta:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"input\": \"Mi piace la boxe!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19868cd9-b9df-4499-80c2-8148c9dcf948",
   "metadata": {},
   "source": [
    "### Oggetti Python con Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                 api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                 temperature=0.7, max_tokens=1024, request_timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ca28e8093c984d74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:52.765077Z",
     "start_time": "2025-02-26T10:36:52.724433Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: int = Field(description=\"user identification number\")\n",
    "    name: str = Field(description=\"user name\")\n",
    "    mail: str = Field(description=\"user mail address\")\n",
    "    \n",
    "    @field_validator(\"mail\")\n",
    "    def is_valid(cls, field):\n",
    "        if not \"@\" in field or \".\" not in field:\n",
    "            raise ValueError(\"Invalid mail\")\n",
    "        return field\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=User)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Analizza il testo\n",
    "{format_instructions}\n",
    "\n",
    "Applica il parser su:\n",
    "{query}\n",
    "\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4a3b9f24f90f47f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:54.010951Z",
     "start_time": "2025-02-26T10:36:54.006409Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"id\": {\"description\": \"user identification number\", \"title\": \"Id\", \"type\": \"integer\"}, \"name\": {\"description\": \"user name\", \"title\": \"Name\", \"type\": \"string\"}, \"mail\": {\"description\": \"user mail address\", \"title\": \"Mail\", \"type\": \"string\"}}, \"required\": [\"id\", \"name\", \"mail\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8d016fdcd3089a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:01.944399Z",
     "start_time": "2025-02-26T10:37:00.510882Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'person'}.  Expected: ['person'] Received: ['query']\\nNote: if you intended {person} to be part of the string and not a variable, please escape it with double curly braces like: '{{person}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[206]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mid: 123456, nominativo: Mario Rossi, e-mail: mario.rossi@gmail.com\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3044\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3042\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3043\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3045\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3046\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:216\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    215\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1939\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1935\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1936\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1937\u001b[39m         output = cast(\n\u001b[32m   1938\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1939\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1947\u001b[39m         )\n\u001b[32m   1948\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1949\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:189\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     inner_input_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**inner_input_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:183\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    177\u001b[39m     example_key = missing.pop()\n\u001b[32m    178\u001b[39m     msg += (\n\u001b[32m    179\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    184\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[31mKeyError\u001b[39m: \"Input to PromptTemplate is missing variables {'person'}.  Expected: ['person'] Received: ['query']\\nNote: if you intended {person} to be part of the string and not a variable, please escape it with double curly braces like: '{{person}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "query = \"id: 123456, nominativo: Mario Rossi, e-mail: mario.rossi@gmail.com\"\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e0fe04e545b0036e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:07.653036Z",
     "start_time": "2025-02-26T10:37:06.118157Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'question'}.  Expected: ['question'] Received: ['query']\\nNote: if you intended {question} to be part of the string and not a variable, please escape it with double curly braces like: '{{question}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[199]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m query = (\u001b[33m\"\u001b[39m\u001b[33mMario 123456 Rossi mario.rossi@gmail.com\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tizio=\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m tizio\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3044\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3042\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3043\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3045\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3046\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:216\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    215\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1939\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1935\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1936\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1937\u001b[39m         output = cast(\n\u001b[32m   1938\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1939\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1947\u001b[39m         )\n\u001b[32m   1948\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1949\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:189\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     inner_input_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**inner_input_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:183\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    177\u001b[39m     example_key = missing.pop()\n\u001b[32m    178\u001b[39m     msg += (\n\u001b[32m    179\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    184\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[31mKeyError\u001b[39m: \"Input to PromptTemplate is missing variables {'question'}.  Expected: ['question'] Received: ['query']\\nNote: if you intended {question} to be part of the string and not a variable, please escape it with double curly braces like: '{{question}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "query = (\"Mario 123456 Rossi mario.rossi@gmail.com\")\n",
    "\n",
    "tizio=chain.invoke({\"query\": query})\n",
    "tizio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a00565f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mario Rossi'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tizio.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "43fb6e3353b919e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:10.090336Z",
     "start_time": "2025-02-26T10:37:09.340475Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User(id=123456, name='Mario', mail='mario.rossi@gmail.com')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = (\"Oggi Mario, che ha il badge n.123456, ha inviato una mail dall'indirizzo mario.rossi@gmail.com\")\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9532690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_emails = \"\"\"\n",
    "Gentile Reparto,\n",
    "\n",
    "stiamo completando una revisione dei dati identificativi dei nostri collaboratori e avremmo bisogno di alcune conferme.\n",
    "\n",
    "In particolare, ci servirebbe verificare:\n",
    "\n",
    "- Il numero di badge associato al sig. Chiapperini (che ci risulta essere 101090).\n",
    "\n",
    "- Le attività principali svolte nell’ultimo trimestre.\n",
    "\n",
    "- La correttezza dei dati di contatto ufficiali.\n",
    "\n",
    "Vi chiediamo cortesemente di rispondere a questa mail entro la fine della settimana.\n",
    "\n",
    "Grazie per la collaborazione,\n",
    "Ufficio Risorse Umane\n",
    "\n",
    "----------------------\n",
    "\n",
    "Mail 2 – Reparto Operativo → Ufficio HR\n",
    "Buongiorno,\n",
    "\n",
    "in riferimento alla vostra richiesta:\n",
    "\n",
    "Il sig. M. Chiapperini risulta registrato con il badge numero 101098, non 101090.\n",
    "\n",
    "Nel corso dell’ultimo trimestre ha gestito:\n",
    "\n",
    "- Supervisione magazzino e logistica.\n",
    "\n",
    "- Coordinamento attività inventariali.\n",
    "\n",
    "- Supporto nella migrazione del sistema gestionale.\n",
    "\n",
    "I dati di contatto risultano confermati: marco.Chiapperini@openrai.com\n",
    " è l’indirizzo principale associato.\n",
    "\n",
    "Cordiali saluti,\n",
    "Responsabile Operativo\n",
    "\n",
    "----------------------\n",
    "\n",
    "Mail 3 – Ufficio HR → Reparto Operativo\n",
    "Grazie della risposta,\n",
    "\n",
    "abbiamo aggiornato i nostri registri interni con il badge 101098 e con le attività riportate.\n",
    "Vorremmo chiedere ancora una conferma: il sig. Chiapperini continuerà a operare nello stesso reparto anche per il prossimo trimestre?\n",
    "\n",
    "Cordiali saluti,\n",
    "Ufficio Risorse Umane\n",
    "\n",
    "----------------------\n",
    "\n",
    "Mail 4 – Reparto Operativo → Ufficio HR\n",
    "Confermiamo che Marco proseguirà con le stesse mansioni nel reparto Logistica e Coordinamento, almeno fino a conclusione del progetto di digitalizzazione.\n",
    "\n",
    "Rimaniamo a disposizione per ulteriori chiarimenti.\n",
    "\n",
    "Distinti saluti,\n",
    "Responsabile Operativo\n",
    "\n",
    "Vuoi che lo trasformi in un testo più realistico e lungo, tipo una vera catena di mail forward/reply (con “Re:” e “Fw:”, date e firme), oppure va bene questa simulazione “pulita” da presentazione?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "806182be",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'input'}.  Expected: ['input'] Received: ['query']\\nNote: if you intended {input} to be part of the string and not a variable, please escape it with double curly braces like: '{{input}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[208]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_emails\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3044\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3042\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3043\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3045\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3046\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:216\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    215\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1939\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1935\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1936\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1937\u001b[39m         output = cast(\n\u001b[32m   1938\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1939\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1947\u001b[39m         )\n\u001b[32m   1948\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1949\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:189\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     inner_input_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**inner_input_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:183\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    177\u001b[39m     example_key = missing.pop()\n\u001b[32m    178\u001b[39m     msg += (\n\u001b[32m    179\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    184\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[31mKeyError\u001b[39m: \"Input to PromptTemplate is missing variables {'input'}.  Expected: ['input'] Received: ['query']\\nNote: if you intended {input} to be part of the string and not a variable, please escape it with double curly braces like: '{{input}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "chain.invoke({\"query\": raw_emails})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf30c352-14be-4e90-bbbf-e3a16d48d338",
   "metadata": {},
   "source": [
    "### JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "25ba92f0ee95337a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:13.111134Z",
     "start_time": "2025-02-26T10:37:11.915210Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 123456, 'name': 'Mario', 'mail': 'mario.rossi@gmail.com'}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=User)\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9a1e5a26795c418a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:16.317955Z",
     "start_time": "2025-02-26T10:37:15.510119Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 123456, 'name': 'Mario', 'mail': 'mario.rossi@gmail.com'}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# riuso delle istruzioni di formattazione di un parser già esistente\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Estrai identificativo, nominativo e indirizzo mail\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe1a74-8baa-4fe2-a395-1b20e70e9dd0",
   "metadata": {},
   "source": [
    "### Pandas Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7510638613a6e9aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:23.142059Z",
     "start_time": "2025-02-26T10:37:22.567154Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.output_parsers import PandasDataFrameOutputParser\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": [\"Canon EOS D60\", \"Agfa ePhoto CL45\", \"Casio QV-R62\", \"Kodak P850\"],\n",
    "        \"max_res\": [3072, 1600, 2816, 2592],\n",
    "        \"eff_pixels\": [6, 1, 4, 5],\n",
    "    }\n",
    ")\n",
    "\n",
    "parser = PandasDataFrameOutputParser(dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "28b1b6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>max_res</th>\n",
       "      <th>eff_pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canon EOS D60</td>\n",
       "      <td>3072</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agfa ePhoto CL45</td>\n",
       "      <td>1600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Casio QV-R62</td>\n",
       "      <td>2816</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kodak P850</td>\n",
       "      <td>2592</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  max_res  eff_pixels\n",
       "0     Canon EOS D60     3072           6\n",
       "1  Agfa ePhoto CL45     1600           1\n",
       "2      Casio QV-R62     2816           4\n",
       "3        Kodak P850     2592           5"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "50eb8fce-f8ee-401a-9e96-a563018dcbf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:25.126468Z",
     "start_time": "2025-02-26T10:37:25.123368Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a string as the operation, followed by a colon, followed by the column or row to be queried on, followed by optional array parameters.\n",
      "1. The column names are limited to the possible columns below.\n",
      "2. Arrays must either be a comma-separated list of numbers formatted as [1,3,5], or it must be in range of numbers formatted as [0..4].\n",
      "3. Remember that arrays are optional and not necessarily required.\n",
      "4. If the column is not in the possible columns or the operation is not a valid Pandas DataFrame operation, return why it is invalid as a sentence starting with either \"Invalid column\" or \"Invalid operation\".\n",
      "\n",
      "As an example, for the formats:\n",
      "1. String \"column:num_legs\" is a well-formatted instance which gets the column num_legs, where num_legs is a possible column.\n",
      "2. String \"row:1\" is a well-formatted instance which gets row 1.\n",
      "3. String \"column:num_legs[1,2]\" is a well-formatted instance which gets the column num_legs for rows 1 and 2, where num_legs is a possible column.\n",
      "4. String \"row:1[num_legs]\" is a well-formatted instance which gets row 1, but for just column num_legs, where num_legs is a possible column.\n",
      "5. String \"mean:num_legs[1..3]\" is a well-formatted instance which takes the mean of num_legs from rows 1 to 3, where num_legs is a possible column and mean is a valid Pandas DataFrame operation.\n",
      "6. String \"do_something:num_legs\" is a badly-formatted instance, where do_something is not a valid Pandas DataFrame operation.\n",
      "7. String \"mean:invalid_col\" is a badly-formatted instance, where invalid_col is not a possible column.\n",
      "\n",
      "Here are the possible columns:\n",
      "```\n",
      "model, max_res, eff_pixels\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "de386a1adf4cd391",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:38:54.921710Z",
     "start_time": "2025-02-26T10:38:54.612320Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    request_timeout=30\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. Attention: don't use double quote where not needed.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c02d774ed49ebbe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:38:56.435057Z",
     "start_time": "2025-02-26T10:38:55.548216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eff_pixels': 0    6\n",
       " 1    1\n",
       " 2    4\n",
       " 3    5\n",
       " Name: eff_pixels, dtype: int64}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query = \"Retrieve the last column.\"\n",
    "\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "parser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3a799683-faaa-4010-9361-6641ab189d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:38:57.905738Z",
     "start_time": "2025-02-26T10:38:57.382207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 0       Canon EOS D60\n",
      "1    Agfa ePhoto CL45\n",
      "2        Casio QV-R62\n",
      "3          Kodak P850\n",
      "Name: model, dtype: object}\n"
     ]
    }
   ],
   "source": [
    "df_query = \"Retrieve the first column.\"\n",
    "\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "\n",
    "print(parser_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6573df4e-5a1d-44f6-b2d0-1bea687aa519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:02.230054Z",
     "start_time": "2025-02-26T10:39:01.688532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': np.float64(2336.0)}\n"
     ]
    }
   ],
   "source": [
    "df_query = \"Recupera la media della colonna max_res dalle righe da 1 a 3.\"\n",
    "\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "\n",
    "print(parser_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb5b44-5ca0-474e-9463-d516368684a7",
   "metadata": {},
   "source": [
    "### Output Strutturato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "49b02664-f5d5-4d4c-8b73-16f7c8f8c106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:10.145259Z",
     "start_time": "2025-02-26T10:39:10.142228Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(  name=\"answer\", description=\"answer to the user's question\"),\n",
    "    ResponseSchema(  name=\"source\", description=\"source used to answer the user's question, should be a website.\",    ),\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8834f6c2-bf62-40f1-89b7-10d64f201077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:22.316047Z",
     "start_time": "2025-02-26T10:39:22.312493Z"
    }
   },
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "87c46b3e-72e5-478a-8cd0-928e5a0ccbdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:24.636202Z",
     "start_time": "2025-02-26T10:39:24.632089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": string  // answer to the user's question\n",
      "\t\"source\": string  // source used to answer the user's question, should be a website.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f4b83f0c-1f74-4565-8512-a3973c1b30c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:27.543042Z",
     "start_time": "2025-02-26T10:39:26.593362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"La capitale dell'Italia è Roma.\",\n",
       " 'source': 'https://www.italia.it'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"question\": \"capitale dell'italia?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd533ff-0239-4f43-8262-da65fb12caa0",
   "metadata": {},
   "source": [
    "### DateTime Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7042d20b-14e4-4588-a704-b20ace24ba02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:29.442760Z",
     "start_time": "2025-02-26T10:39:29.438625Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "template = \"\"\"Answer the users question:\n",
    "\n",
    "{question}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "93e96c4c-9a2a-49dd-8564-b21735cbea09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:30.351554Z",
     "start_time": "2025-02-26T10:39:30.346452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n",
      "\n",
      "Examples: 0692-04-14T09:49:34.888462Z, 0155-10-17T01:44:44.140418Z, 0619-02-24T17:01:26.448376Z\n",
      "\n",
      "Return ONLY this string, no other words!\n"
     ]
    }
   ],
   "source": [
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "13c162fe-1aef-469c-af39-77657c9f41e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:32.513209Z",
     "start_time": "2025-02-26T10:39:31.888160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1955-02-24 00:00:00\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "output = chain.invoke({\"question\": \"Quando è nato Steve Jobs?\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6771d-3dd7-4319-9181-f888c36ac29b",
   "metadata": {},
   "source": [
    "### Enum Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e00e1676-9d6d-4b65-bf97-8daaa453db31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:37.733402Z",
     "start_time": "2025-02-26T10:39:36.485412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colors.GREEN\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from enum import Enum\n",
    "\n",
    "class Colors(Enum):\n",
    "    BLACK = \"Nero\"\n",
    "    GREEN = \"Verde\"\n",
    "    BLUE = \"Blu\"\n",
    "    GRAY = \"Grigio\"\n",
    "    BROWN = \"Marrone\"\n",
    "\n",
    "parser = EnumOutputParser(enum=Colors)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Dimmi qual è il colore degli occhi di questa persona?\n",
    "\n",
    "> Persona: {person}\n",
    "\n",
    "Attenzione! Atteiniti stettamente a queste istruzioni: {instructions} seleziona sola una delle opzioni\"\"\"\n",
    ").partial(instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "print(chain.invoke({\"person\": \"Fabrizio De André\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eee01e-2717-4f26-97e6-e396783c42b6",
   "metadata": {},
   "source": [
    "### Lista di valori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a046bce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select one of the following options: Nero, Verde, Blu, Grigio, Marrone'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d397c3-c6c0-442c-bdc0-1545a201ad31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:39.776084Z",
     "start_time": "2025-02-26T10:39:39.772504Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432d952-293e-4525-9420-677b08ad4efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:41.833423Z",
     "start_time": "2025-02-26T10:39:41.828812Z"
    }
   },
   "outputs": [],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4364ec-c4db-4280-be77-5f0df0aadea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:43.492480Z",
     "start_time": "2025-02-26T10:39:42.764476Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "print(chain.invoke({\"subject\": \"elenca i pianeti del sistema solare in ordine dal più vicino al più lontano dal Sole\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24187344",
   "metadata": {},
   "source": [
    "### Gradio Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31aa6be-1dce-48be-9ccb-16055f674840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def echo(message, history):\n",
    "    return message\n",
    "\n",
    "demo = gr.ChatInterface(fn=echo, type=\"messages\", examples=[\"hello\", \"ciao\", \"salve\"], title=\"Echo Bot\")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
