{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b2ea032d525cef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Introduzione a LangChain\n",
    "\n",
    "***Argomenti:***\n",
    "* Prompt Template\n",
    "* Runnable\n",
    "* Memoria Sequenziale\n",
    "* Parserizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45715b13d443d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:26:42.324182Z",
     "start_time": "2025-02-26T10:26:42.000778Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI  # pip install langchain-openai\n",
    "\n",
    "# Connettore verso il modello di linguaggio OpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",                            # modello di linguaggio da utilizzare\n",
    "    openai_api_key=os.getenv(\"openai_api_key\"),     # chiave API OpenAI\n",
    "    temperature=.7,                                 # controlla la creativit√† del modello\n",
    "    max_tokens=1024,                                # numero massimo di token nella risposta    \n",
    "    request_timeout=30                              # timeout della richiesta in secondi\n",
    "\n",
    "    # Per utilizzare un connettore per modelli in locale, puoi usare:\n",
    "    #model = \"meta-llama/Llama-3.2-3B-Instruct\",      # esempio di modello locale attraverso LM Studio\n",
    "    #base_url = \"http://127.0.0.1:8000/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf358b",
   "metadata": {},
   "source": [
    "## Connettore al locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84433b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama         # pip install langchain-ollama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama-vincent\",\n",
    "    temperature=0.7,\n",
    "    num_predict=1024,\n",
    "    base_url=\"http://localhost:11434\",  # opzionale\n",
    "    request_timeout=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"crea un prompt per l'immagine in allegato in modo da trasformare la persona nell'immagine in allegato in un personaggio fantasy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e02bb220c9b50",
   "metadata": {},
   "source": [
    "## üß† OpenAI GPT-4o mini\n",
    "\n",
    "**GPT-4o mini** (*Generative Pre-trained Transformer 4o mini*) √® una versione compatta e ottimizzata del modello **GPT-4o**, introdotta da **OpenAI nel 2024**.\n",
    "\n",
    "Progettato per **massimizzare l'efficienza computazionale** mantenendo elevate prestazioni conversazionali, GPT-4o mini rappresenta un compromesso ideale tra **potenza, velocit√† e costi operativi**, risultando perfetto per applicazioni real-time, mobile, embedded o cloud a basso consumo.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Caratteristiche tecniche principali\n",
    "\n",
    "| Caratteristica                         | Dettaglio                                         |\n",
    "|----------------------------------------|--------------------------------------------------|\n",
    "| **Architettura**                       | Transformer Decoder-only                         |\n",
    "| **Context window**                     | Fino a **128K token**                            |\n",
    "| **Token generabili per output**        | Fino a **16.384 token**                          |\n",
    "| **Modalit√† input/output**              | Testo, codice, immagini, audio (come GPT-4o)     |\n",
    "| **Tecnica di addestramento**          | RLHF (*Reinforcement Learning from Human Feedback*) |\n",
    "| **Performance/costo**                 | Ottimizzato per ambienti a risorse limitate      |\n",
    "| **Compatibilit√† API**                 | Interfaccia OpenAI compatibile (`/v1/chat/completions`) |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Vantaggi d‚Äôuso\n",
    "\n",
    "- üîã **Basso consumo di risorse**: adatto per dispositivi edge, inferenza locale o istanze cloud leggere.  \n",
    "- üöÄ **Alta velocit√† di risposta**: eccellente per sistemi in tempo reale (es. agenti, chatbot, copiloti).  \n",
    "- üí∞ **Costo ridotto**: bilanciamento ideale per applicazioni in larga scala o multitenant.  \n",
    "- üß© **Facilmente integrabile**: piena compatibilit√† con l'ecosistema OpenAI, LangChain e agent frameworks.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Quando usarlo\n",
    "\n",
    "GPT-4o mini √® particolarmente indicato per:\n",
    "\n",
    "- App AI-powered con migliaia/milioni di utenti simultanei  \n",
    "- Interfacce conversazionali rapide (es. helpdesk, tutor virtuali)  \n",
    "- Sistemi embedded, mobile o con requisiti real-time  \n",
    "- Test e prototipazione low-cost di agenti AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fb51e59495905",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Tecniche di Addestramento dei LLMs  \n",
    "### üéØ Fine-tuning vs RLHF (Reinforcement Learning from Human Feedback)\n",
    "\n",
    "L‚Äôaddestramento dei modelli linguistici di grandi dimensioni (*LLMs*) pu√≤ avvenire tramite due approcci principali: **Fine-tuning** e **RLHF**. Sebbene entrambi puntino a migliorare le prestazioni del modello, differiscono profondamente nella metodologia e negli obiettivi.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Fine-tuning\n",
    "\n",
    "Nel **fine-tuning**, il modello viene ulteriormente addestrato su un **dataset statico** di esempi *input-output* gi√† etichettati. Durante questo processo:\n",
    "\n",
    "- ‚úÖ I **pesi del modello** vengono aggiornati per adattarsi ai dati specifici.\n",
    "- üéØ Il modello **imita** gli esempi forniti, replicando schemi e risposte.\n",
    "- üß† √à un processo **diretto e supervisionato**, ideale per:\n",
    "  - Domini specifici (es. medicina, finanza)\n",
    "  - Task precisi (es. classificazione, Q&A tecnico)\n",
    "\n",
    "> üìå Il modello *non apprende attivamente dal feedback umano*, ma si adatta passivamente ai dati forniti.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† RLHF ‚Äî Reinforcement Learning from Human Feedback\n",
    "\n",
    "Il **RLHF** √® un approccio **pi√π interattivo e iterativo**, in cui il modello apprende sulla base delle **preferenze espresse dagli esseri umani**.\n",
    "\n",
    "1. Il modello genera pi√π risposte per uno stesso input.\n",
    "2. Gli umani **valutano le risposte**, indicando quale preferiscono.\n",
    "3. Un **modello di ricompensa** converte questo feedback in **punteggi numerici**.\n",
    "4. Un algoritmo di **reinforcement learning** (es. PPO) aggiorna i pesi del modello, spingendolo verso risposte pi√π coerenti con i desideri umani.\n",
    "\n",
    "> üîÅ Questo processo consente **adattamenti graduali e continui**, favorendo un comportamento pi√π allineato a **valori, etica e qualit√† percepita dall‚Äôutente**.\n",
    "\n",
    "---\n",
    "\n",
    "### üÜö Confronto rapido\n",
    "\n",
    "| Aspetto                | Fine-tuning                           | RLHF                                         |\n",
    "|------------------------|----------------------------------------|----------------------------------------------|\n",
    "| Tipo di dati           | Dataset statico etichettato           | Feedback umano in tempo reale                |\n",
    "| Modalit√†               | Supervisato                           | Interattivo, basato su ricompensa            |\n",
    "| Obiettivo              | Adattamento a compiti o domini        | Allineamento a preferenze e valori umani     |\n",
    "| Apprendimento          | Diretto                               | Iterativo e adattivo                         |\n",
    "| Complessit√†            | Pi√π semplice                          | Pi√π complesso (richiede interazione umana)   |\n",
    "\n",
    "---\n",
    "\n",
    "üî¨ **Conclusione**  \n",
    "Mentre il fine-tuning √® ideale per specializzare un modello, il RLHF √® cruciale per ottenere **comportamenti generativi pi√π umani e sicuri**, particolarmente nei modelli conversazionali avanzati come ChatGPT, Claude o Gemini.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38330f141f9c6c6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:26:45.214780Z",
     "start_time": "2025-02-26T10:26:44.059968Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "llm.invoke(\"La nebbia agli irti colli...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362151d",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8437aefa5d85256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:26:50.417139Z",
     "start_time": "2025-02-26T10:26:50.409530Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate  # pip install langchain\n",
    "\n",
    "# creazione di un template di prompt per il modello\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Act as a novelist from the Romantic era, yet highly knowledgeable about contemporary topics. Use highly technical terms wherever possible. Answer accordig to the user language\"),\n",
    "    (\"user\", \"{un_placeholder}\"), # tra parentesi graffe va il placeholder che verr√† riempito con il contenuto e pu√≤ avere un nome qualsiasi\n",
    "    # (\"user\", \"Scrivi un racconto breve su un tema romantico su {un_placeholder}, ma con riferimenti a {un_altro_placeholder}.\"),\n",
    "])\n",
    "\n",
    "# concatenazione del prompt al modello tramite il carattere pipe |\n",
    "# il simbolo | deriva da langchain expression language (LCEL), che permette di concatenare oggetti\n",
    "chain = prompt | llm  # il prompt (ChatPromptTemplate) va =>  nel modello (ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d639f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.invoke({\"un_placeholder\": \"Cos'√® chatGPT?\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07be2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b326fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=chain.invoke({\"un_placeholder\": \"Cos'√® chatGPT?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa049fbcb0821047",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:27:42.411689Z",
     "start_time": "2025-02-26T10:27:41.187281Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result=chain.invoke({\"un_placeholder\": \"qual √® il modello di machine learning pi√π adatto per risolvere un problema di classificazione?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c669b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimuoviamo il placeholder per l'utente\n",
    "result=chain.invoke({\"quali sono le metriche di valutazione pi√π comuni per un modello di classificazione?\"})  # questa volta manca il \"placeholder\" per user\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86adbabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.response_metadata.get(\"token_usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f01ffb-98fe-4833-95b6-a4a3a265908b",
   "metadata": {},
   "source": [
    "## Interfaccia Runnable\n",
    "\n",
    "Per rendere pi√π semplice la creazione di catene di eventi/esecuzione anche molto complesse i componenti di LangChain implementano tutti un protocollo \"runnable\" tramite un'interfaccia comune che permette di usare qualsiasi componente in modo standard; di seguito sono elencati i 3 principali metodi:\n",
    "\n",
    "* **stream** - inviare risposte parziali mentre vengono generate\n",
    "* **invoke** - eseguire la catena su un input\n",
    "* **batch** - esecuzione della catena su pi√π input\n",
    "\n",
    "Uno dei vantaggi delle interfacce Runnable √® dato dal fatto che dei componenti *runnable* possono essere concatenati in sequenze di esecuzione, facendo in modo che, automaticamente, gli output di un componente possano entrare in input ad un altro; il comando *pipe* `|` serve a questo e permette, nella sintassi LCEL (LangChain Expression Language) di creare componenti runnable partendo da altri componenti runnable, configurandoli in una sequenza di componenti che agiranno sinergicamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27a9d8-5331-4499-bd8f-d6ee1b6ed927",
   "metadata": {},
   "source": [
    "   \n",
    "[https://python.langchain.com/docs/concepts/messages/#langchain-messages](https://python.langchain.com/docs/concepts/messages/#langchain-messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40058641",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.stream(\"Parlami delle GPU e del loro utilizzo nel deep learning\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Ciao, il mio cantate preferito √® Lucio Battisti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7be4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Chi √® il mio cantante preferito?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ac851",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d08484d403ee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:29:18.197131Z",
     "start_time": "2025-02-26T10:29:18.088003Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# memoria sequenziale di una conversazione\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"Buongiono\")\n",
    "memory.chat_memory.add_ai_message(\"Ciao, come va?\")\n",
    "memory.chat_memory.add_user_message(\"Tutto ok, grazie. A te?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c542c4f5dacaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:31:36.413579Z",
     "start_time": "2025-02-26T10:31:36.408854Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "history = memory.load_memory_variables({})\n",
    "print(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a596a139cda061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:35:44.003141Z",
     "start_time": "2025-02-26T10:35:43.998594Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True) # impostiamo return_messages=True per ottenere i messaggi della conversazione in maniera composta\n",
    "\n",
    "# memory.chat_memory.clear() per svuotare la memoria conversazionale\n",
    "memory.chat_memory.add_user_message(\"ciao\")\n",
    "memory.chat_memory.add_ai_message(\"ciao, come v√†?\")\n",
    "memory.chat_memory.add_user_message(\"mah, non benissimo, ho un problema con il mio codice javascript\")\n",
    "memory.chat_memory.add_ai_message(\"mi spiace uso python\")\n",
    "\n",
    "\n",
    "\n",
    "history = memory.load_memory_variables(\"\")\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab4892",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in history['history']:\n",
    "    print(i.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75174e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.get(\"history\")[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e32428d170395",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:33.664150Z",
     "start_time": "2025-02-26T10:36:33.654478Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# il messaggio di sistema per il modello √® opzionale, ma √® utile per fornire un contesto specifico\n",
    "system_template = \"Agisci come un esperto AI engineer e cerca sempre di fare collegamenti al contesto AI.\"\n",
    "\n",
    "# includiamo la memoria nella conversazione\n",
    "# MessagesPlaceholder √® un segnaposto per i messaggi della memoria conversazionale\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        MessagesPlaceholder(variable_name=\"history\"), # qui iniettiamo tutta la conversazione passata\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# importiamo le classi necessarie per la memoria conversazionale\n",
    "# impostiamo memory_key=\"history\" per indicare che la memoria conversazionale sar√† associata alla chiave \"history\"\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
    "\n",
    "# ATTENZIONE:\n",
    "# creiamo la conversazione utilizzando RunnablePassthrough e RunnableLambda\n",
    "# RunnablePassthrough permette di passare i dati attraverso la pipeline senza modificarli\n",
    "# RunnableLambda permette di eseguire una funzione su di essi.\n",
    "conversation = (\n",
    "        # questo √® il passaggio della memoria conversazionale\n",
    "        RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))\n",
    "        | chat_prompt\n",
    "        | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581bf44",
   "metadata": {},
   "source": [
    "1. RunnablePassthrough.assign(...)\n",
    "    - RunnablePassthrough √® un runnable (un oggetto composabile) che tipicamente passa i dati inalterati, ma con .assign() puoi arricchire i dati con nuove chiavi.\n",
    "\n",
    "    - .assign(history=...) significa che aggiunger√† (o sovrascriver√†) la chiave \"history\" nel dizionario che passa lungo la pipeline\n",
    "\n",
    "2. history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    - RunnableLambda(memory.load_memory_variables) prende una funzione (memory.load_memory_variables) e la rende compatibile con la pipeline dei runnable.\n",
    "\n",
    "    - La funzione memory.load_memory_variables restituisce un dizionario che contiene la storia della conversazione (e forse anche altri dati).\n",
    "\n",
    "    - La pipe | compone due runnable: il risultato di RunnableLambda(...) viene passato a itemgetter(\"history\").\n",
    "\n",
    "        - itemgetter(\"history\") prende un dizionario in ingresso e estrae il valore associato alla chiave \"history\".\n",
    "\n",
    "3. Comportamento finale\n",
    "Quando la pipeline √® attivata:\n",
    "\n",
    "    - Verranno passati i dati (ad esempio, l‚Äôinput utente).\n",
    "\n",
    "    - La chiave \"history\" sar√† iniettata nel dizionario e conterr√† solo il valore specifico della storia conversazionale, estratto dalla memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cbc2d1e91261a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:37.310022Z",
     "start_time": "2025-02-26T10:36:35.054656Z"
    }
   },
   "outputs": [],
   "source": [
    "msg_input = {\"input\": \"ciao, mi piace il rock anni '70!\"}\n",
    "result = conversation.invoke(msg_input)\n",
    "\n",
    "# aggiorno la memoria\n",
    "memory.chat_memory.add_user_message(msg_input[\"input\"])\n",
    "memory.chat_memory.add_ai_message(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc4e3cf240e0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:37.327768Z",
     "start_time": "2025-02-26T10:36:37.324195Z"
    }
   },
   "outputs": [],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d264e7c830a214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:42.564134Z",
     "start_time": "2025-02-26T10:36:37.464833Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "msg_input = {\"input\": \"elencami tre artisti che hanno contraddistinto questo genere musicale\"}\n",
    "\n",
    "result = conversation.invoke(msg_input)\n",
    "\n",
    "# aggiorno la memoria\n",
    "memory.chat_memory.add_user_message(msg_input[\"input\"])\n",
    "memory.chat_memory.add_ai_message(result.content)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e9fce960f5547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:42.656412Z",
     "start_time": "2025-02-26T10:36:42.653386Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Debugging della memoria\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory  # in-memory semplice\n",
    "\n",
    "# 1) Chat Prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        MessagesPlaceholder(variable_name=\"history\"), # qui iniettiamo tutta la conversazione passata\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2) Chain\n",
    "base_chain = chat_prompt | llm \n",
    "\n",
    "# 3) Store per history per-sessione\n",
    "store = {}  # sostituisci con Redis, DB, ecc. in produzione\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 4) Wrappa la catena con la gestione automatica della history\n",
    "chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",     # chiave dell‚Äôinput utente\n",
    "    history_messages_key=\"history\", # deve matchare il MessagesPlaceholder\n",
    "    # output_messages_key √® opzionale: se omesso, salva l'AI message di default\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9063a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Uso\n",
    "session_id = \"demo-123\"  # es. id utente o thread\n",
    "res1 = chain.invoke(\n",
    "    {\"input\": \"ciao, mi piace il rock anni '70!\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "print(res1.content)\n",
    "\n",
    "# Nuovo turno: la history √® gi√† aggiornata automaticamente\n",
    "res2 = chain.invoke(\n",
    "    {\"input\": \"Suggeriscimi 3 band e perch√©.\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1426092d996862e",
   "metadata": {},
   "source": [
    "<hr>    \n",
    "    \n",
    "# Parserizzazione degli Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7e578ee8dce0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:48.486342Z",
     "start_time": "2025-02-26T10:36:46.214621Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Agisci come un esperto Frontend Developer e concludi le tue frasi facendo riferimento all'importanza dell'accessibilit√† nel design.\n",
    "Domanda: {input}\n",
    "Risposta:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"input\": \"Mi piace la boxe!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19868cd9-b9df-4499-80c2-8148c9dcf948",
   "metadata": {},
   "source": [
    "### Oggetti Python con Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                 api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                 temperature=0.7, max_tokens=1024, request_timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca28e8093c984d74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:52.765077Z",
     "start_time": "2025-02-26T10:36:52.724433Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: int = Field(description=\"user identification number\")\n",
    "    name: str = Field(description=\"user name\")\n",
    "    mail: str = Field(description=\"user mail address\")\n",
    "    \n",
    "    @field_validator(\"mail\")\n",
    "    def is_valid(cls, field):\n",
    "        if not \"@\" in field or \".\" not in field:\n",
    "            raise ValueError(\"Invalid mail\")\n",
    "        return field\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=User)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Analizza il testo\n",
    "{format_instructions}\n",
    "\n",
    "Applica il parser su:\n",
    "{query}\n",
    "\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b9f24f90f47f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:36:54.010951Z",
     "start_time": "2025-02-26T10:36:54.006409Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d016fdcd3089a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:01.944399Z",
     "start_time": "2025-02-26T10:37:00.510882Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User(id=123456, name='Mario Rossi', mail='mario.rossi@gmail.com')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"id: 123456, nominativo: Mario Rossi, e-mail: mario.rossi@gmail.com\"\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0fe04e545b0036e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:07.653036Z",
     "start_time": "2025-02-26T10:37:06.118157Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User(id=123456, name='Mario Rossi', mail='mario.rossi@gmail.com')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = (\"Mario 123456 Rossi mario.rossi@gmail.com\")\n",
    "\n",
    "tizio=chain.invoke({\"query\": query})\n",
    "tizio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a00565f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mario Rossi'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tizio.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43fb6e3353b919e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:10.090336Z",
     "start_time": "2025-02-26T10:37:09.340475Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User(id=123456, name='Mario', mail='mario.rossi@gmail.com')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = (\"Oggi Mario, che ha il badge n.123456, ha inviato una mail dall'indirizzo mario.rossi@gmail.com\")\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9532690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_emails = \"\"\"\n",
    "Gentile Reparto,\n",
    "\n",
    "stiamo completando una revisione dei dati identificativi dei nostri collaboratori e avremmo bisogno di alcune conferme.\n",
    "\n",
    "In particolare, ci servirebbe verificare:\n",
    "\n",
    "- Il numero di badge associato al sig. Chiapperini (che ci risulta essere 101090).\n",
    "\n",
    "- Le attivit√† principali svolte nell‚Äôultimo trimestre.\n",
    "\n",
    "- La correttezza dei dati di contatto ufficiali.\n",
    "\n",
    "Vi chiediamo cortesemente di rispondere a questa mail entro la fine della settimana.\n",
    "\n",
    "Grazie per la collaborazione,\n",
    "Ufficio Risorse Umane\n",
    "\n",
    "----------------------\n",
    "\n",
    "Mail 2 ‚Äì Reparto Operativo ‚Üí Ufficio HR\n",
    "Buongiorno,\n",
    "\n",
    "in riferimento alla vostra richiesta:\n",
    "\n",
    "Il sig. M. Chiapperini risulta registrato con il badge numero 101098, non 101090.\n",
    "\n",
    "Nel corso dell‚Äôultimo trimestre ha gestito:\n",
    "\n",
    "- Supervisione magazzino e logistica.\n",
    "\n",
    "- Coordinamento attivit√† inventariali.\n",
    "\n",
    "- Supporto nella migrazione del sistema gestionale.\n",
    "\n",
    "I dati di contatto risultano confermati: marco.Chiapperini@openrai.com\n",
    " √® l‚Äôindirizzo principale associato.\n",
    "\n",
    "Cordiali saluti,\n",
    "Responsabile Operativo\n",
    "\n",
    "----------------------\n",
    "\n",
    "Mail 3 ‚Äì Ufficio HR ‚Üí Reparto Operativo\n",
    "Grazie della risposta,\n",
    "\n",
    "abbiamo aggiornato i nostri registri interni con il badge 101098 e con le attivit√† riportate.\n",
    "Vorremmo chiedere ancora una conferma: il sig. Chiapperini continuer√† a operare nello stesso reparto anche per il prossimo trimestre?\n",
    "\n",
    "Cordiali saluti,\n",
    "Ufficio Risorse Umane\n",
    "\n",
    "----------------------\n",
    "\n",
    "Mail 4 ‚Äì Reparto Operativo ‚Üí Ufficio HR\n",
    "Confermiamo che Marco proseguir√† con le stesse mansioni nel reparto Logistica e Coordinamento, almeno fino a conclusione del progetto di digitalizzazione.\n",
    "\n",
    "Rimaniamo a disposizione per ulteriori chiarimenti.\n",
    "\n",
    "Distinti saluti,\n",
    "Responsabile Operativo\n",
    "\n",
    "Vuoi che lo trasformi in un testo pi√π realistico e lungo, tipo una vera catena di mail forward/reply (con ‚ÄúRe:‚Äù e ‚ÄúFw:‚Äù, date e firme), oppure va bene questa simulazione ‚Äúpulita‚Äù da presentazione?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "806182be",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'question'}.  Expected: ['question'] Received: ['query']\\nNote: if you intended {question} to be part of the string and not a variable, please escape it with double curly braces like: '{{question}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_emails\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3044\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3042\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3043\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3045\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3046\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:216\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    215\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1939\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1935\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1936\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1937\u001b[39m         output = cast(\n\u001b[32m   1938\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1939\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1947\u001b[39m         )\n\u001b[32m   1948\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1949\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:189\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     inner_input_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**inner_input_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vinor\\Desktop\\Develhope\\projects\\EDU-LLMs_intro\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:183\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    177\u001b[39m     example_key = missing.pop()\n\u001b[32m    178\u001b[39m     msg += (\n\u001b[32m    179\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    184\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[31mKeyError\u001b[39m: \"Input to PromptTemplate is missing variables {'question'}.  Expected: ['question'] Received: ['query']\\nNote: if you intended {question} to be part of the string and not a variable, please escape it with double curly braces like: '{{question}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "chain.invoke({\"query\": raw_emails})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf30c352-14be-4e90-bbbf-e3a16d48d338",
   "metadata": {},
   "source": [
    "### JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "25ba92f0ee95337a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:13.111134Z",
     "start_time": "2025-02-26T10:37:11.915210Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 123456, 'name': 'Mario', 'mail': 'mario.rossi@gmail.com'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=User)\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a1e5a26795c418a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:16.317955Z",
     "start_time": "2025-02-26T10:37:15.510119Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 123456, 'name': 'Mario', 'mail': 'mario.rossi@gmail.com'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# riuso delle istruzioni di formattazione di un parser gi√† esistente\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Estrai identificativo, nominativo e indirizzo mail\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe1a74-8baa-4fe2-a395-1b20e70e9dd0",
   "metadata": {},
   "source": [
    "### Pandas Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7510638613a6e9aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:23.142059Z",
     "start_time": "2025-02-26T10:37:22.567154Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.output_parsers import PandasDataFrameOutputParser\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": [\"Canon EOS D60\", \"Agfa ePhoto CL45\", \"Casio QV-R62\", \"Kodak P850\"],\n",
    "        \"max_res\": [3072, 1600, 2816, 2592],\n",
    "        \"eff_pixels\": [6, 1, 4, 5],\n",
    "    }\n",
    ")\n",
    "\n",
    "parser = PandasDataFrameOutputParser(dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28b1b6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>max_res</th>\n",
       "      <th>eff_pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canon EOS D60</td>\n",
       "      <td>3072</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agfa ePhoto CL45</td>\n",
       "      <td>1600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Casio QV-R62</td>\n",
       "      <td>2816</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kodak P850</td>\n",
       "      <td>2592</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  max_res  eff_pixels\n",
       "0     Canon EOS D60     3072           6\n",
       "1  Agfa ePhoto CL45     1600           1\n",
       "2      Casio QV-R62     2816           4\n",
       "3        Kodak P850     2592           5"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50eb8fce-f8ee-401a-9e96-a563018dcbf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:37:25.126468Z",
     "start_time": "2025-02-26T10:37:25.123368Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a string as the operation, followed by a colon, followed by the column or row to be queried on, followed by optional array parameters.\n",
      "1. The column names are limited to the possible columns below.\n",
      "2. Arrays must either be a comma-separated list of numbers formatted as [1,3,5], or it must be in range of numbers formatted as [0..4].\n",
      "3. Remember that arrays are optional and not necessarily required.\n",
      "4. If the column is not in the possible columns or the operation is not a valid Pandas DataFrame operation, return why it is invalid as a sentence starting with either \"Invalid column\" or \"Invalid operation\".\n",
      "\n",
      "As an example, for the formats:\n",
      "1. String \"column:num_legs\" is a well-formatted instance which gets the column num_legs, where num_legs is a possible column.\n",
      "2. String \"row:1\" is a well-formatted instance which gets row 1.\n",
      "3. String \"column:num_legs[1,2]\" is a well-formatted instance which gets the column num_legs for rows 1 and 2, where num_legs is a possible column.\n",
      "4. String \"row:1[num_legs]\" is a well-formatted instance which gets row 1, but for just column num_legs, where num_legs is a possible column.\n",
      "5. String \"mean:num_legs[1..3]\" is a well-formatted instance which takes the mean of num_legs from rows 1 to 3, where num_legs is a possible column and mean is a valid Pandas DataFrame operation.\n",
      "6. String \"do_something:num_legs\" is a badly-formatted instance, where do_something is not a valid Pandas DataFrame operation.\n",
      "7. String \"mean:invalid_col\" is a badly-formatted instance, where invalid_col is not a possible column.\n",
      "\n",
      "Here are the possible columns:\n",
      "```\n",
      "model, max_res, eff_pixels\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de386a1adf4cd391",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:38:54.921710Z",
     "start_time": "2025-02-26T10:38:54.612320Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    request_timeout=30\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. Attention: don't use double quote where not needed.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c02d774ed49ebbe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:38:56.435057Z",
     "start_time": "2025-02-26T10:38:55.548216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eff_pixels': 0    6\n",
       " 1    1\n",
       " 2    4\n",
       " 3    5\n",
       " Name: eff_pixels, dtype: int64}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query = \"Retrieve the last column.\"\n",
    "\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "parser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3a799683-faaa-4010-9361-6641ab189d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:38:57.905738Z",
     "start_time": "2025-02-26T10:38:57.382207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 0       Canon EOS D60\n",
      "1    Agfa ePhoto CL45\n",
      "2        Casio QV-R62\n",
      "3          Kodak P850\n",
      "Name: model, dtype: object}\n"
     ]
    }
   ],
   "source": [
    "df_query = \"Retrieve the first column.\"\n",
    "\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "\n",
    "print(parser_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6573df4e-5a1d-44f6-b2d0-1bea687aa519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:02.230054Z",
     "start_time": "2025-02-26T10:39:01.688532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': np.float64(2336.0)}\n"
     ]
    }
   ],
   "source": [
    "df_query = \"Recupera la media della colonna max_res dalle righe da 1 a 3.\"\n",
    "\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "\n",
    "print(parser_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb5b44-5ca0-474e-9463-d516368684a7",
   "metadata": {},
   "source": [
    "### Output Strutturato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "49b02664-f5d5-4d4c-8b73-16f7c8f8c106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:10.145259Z",
     "start_time": "2025-02-26T10:39:10.142228Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(  name=\"answer\", description=\"answer to the user's question\"),\n",
    "    ResponseSchema(  name=\"source\", description=\"source used to answer the user's question, should be a website.\",    ),\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8834f6c2-bf62-40f1-89b7-10d64f201077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:22.316047Z",
     "start_time": "2025-02-26T10:39:22.312493Z"
    }
   },
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "87c46b3e-72e5-478a-8cd0-928e5a0ccbdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:24.636202Z",
     "start_time": "2025-02-26T10:39:24.632089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": string  // answer to the user's question\n",
      "\t\"source\": string  // source used to answer the user's question, should be a website.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f4b83f0c-1f74-4565-8512-a3973c1b30c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:27.543042Z",
     "start_time": "2025-02-26T10:39:26.593362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"La capitale dell'Italia √® Roma.\",\n",
       " 'source': 'https://www.italia.it'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"question\": \"capitale dell'italia?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd533ff-0239-4f43-8262-da65fb12caa0",
   "metadata": {},
   "source": [
    "### DateTime Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7042d20b-14e4-4588-a704-b20ace24ba02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:29.442760Z",
     "start_time": "2025-02-26T10:39:29.438625Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "template = \"\"\"Answer the users question:\n",
    "\n",
    "{question}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "93e96c4c-9a2a-49dd-8564-b21735cbea09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:30.351554Z",
     "start_time": "2025-02-26T10:39:30.346452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n",
      "\n",
      "Examples: 1492-10-15T16:40:11.965271Z, 1391-09-04T06:39:56.445358Z, 0625-07-14T04:01:01.991688Z\n",
      "\n",
      "Return ONLY this string, no other words!\n"
     ]
    }
   ],
   "source": [
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "13c162fe-1aef-469c-af39-77657c9f41e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:32.513209Z",
     "start_time": "2025-02-26T10:39:31.888160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1955-02-24 00:00:00\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "output = chain.invoke({\"question\": \"Quando √® nato Steve Jobs?\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6771d-3dd7-4319-9181-f888c36ac29b",
   "metadata": {},
   "source": [
    "### Enum Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e00e1676-9d6d-4b65-bf97-8daaa453db31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:37.733402Z",
     "start_time": "2025-02-26T10:39:36.485412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colors.BLUE\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from enum import Enum\n",
    "\n",
    "class Colors(Enum):\n",
    "    BLACK = \"Nero\"\n",
    "    GREEN = \"Verde\"\n",
    "    BLUE = \"Blu\"\n",
    "    GRAY = \"Grigio\"\n",
    "    BROWN = \"Marrone\"\n",
    "\n",
    "parser = EnumOutputParser(enum=Colors)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Dimmi qual √® il colore degli occhi di questa persona?\n",
    "\n",
    "> Persona: {person}\n",
    "\n",
    "Attenzione! Atteiniti stettamente a queste istruzioni: {instructions} seleziona sola una delle opzioni\"\"\"\n",
    ").partial(instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "print(chain.invoke({\"person\": \"Fabrizio De Andr√©\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a046bce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select one of the following options: Nero, Verde, Blu, Grigio, Marrone'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eee01e-2717-4f26-97e6-e396783c42b6",
   "metadata": {},
   "source": [
    "### Lista di valori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "07d397c3-c6c0-442c-bdc0-1545a201ad31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:39.776084Z",
     "start_time": "2025-02-26T10:39:39.772504Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1432d952-293e-4525-9420-677b08ad4efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:41.833423Z",
     "start_time": "2025-02-26T10:39:41.828812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de4364ec-c4db-4280-be77-5f0df0aadea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T10:39:43.492480Z",
     "start_time": "2025-02-26T10:39:42.764476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mercurio', 'Venere', 'Terra', 'Marte', 'Giove', 'Saturno', 'Urano', 'Nettuno']\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "print(chain.invoke({\"subject\": \"elenca i pianeti del sistema solare in ordine dal pi√π vicino al pi√π lontano dal Sole\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24187344",
   "metadata": {},
   "source": [
    "### Gradio Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31aa6be-1dce-48be-9ccb-16055f674840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def echo(message, history):\n",
    "    return message\n",
    "\n",
    "demo = gr.ChatInterface(fn=echo, type=\"messages\", examples=[\"hello\", \"ciao\", \"salve\"], title=\"Echo Bot\")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
